# Arabic Documents Summarization, NER & Topic Modeling

A comprehensive Arabic NLP pipeline that combines multiple state-of-the-art models for text summarization, Named Entity Recognition, sentiment analysis, and topic modeling with detailed benchmarking and evaluation metrics (No LLM).

## Overview

This project implements a production-ready Arabic Natural Language Processing pipeline that handles the complete NLP workflow:

1. **Text Preprocessing**: Arabic normalization, tokenization, and lemmatization
2. **Named Entity Recognition**: Multi-model comparison (CAMeL, Hatmimoha, Stanza)
3. **Summarization**: Both extractive (Sumy) and abstractive (AraBART, mT5-XLSum)
4. **Sentiment Analysis**: Document-level sentiment classification
5. **Topic Modeling**: Automatic topic extraction with coherence scoring

All components include:

- âœ… Automatic evaluation metrics
- âœ… Multi-model comparison for fair benchmarking
- âœ… Real-world Arabic datasets with annotations
- âœ… Graceful error handling and model fallbacks

---

## Features

### Text Preprocessing

- **Unicode Normalization**: Standardizes Arabic characters
- **Diacritical Removal**: Removes Tasdeed, Fatha, Damma, etc.
- **Character Normalization**:
  - Alef variants â†’ Ø§
  - Maksura â†’ ÙŠ
  - Teh Marbuta â†’ Ø©
- **Lemmatization**: CAMeL morphological database analysis
- **Stopword Removal**: 100+ Arabic stopwords filtered

### Named Entity Recognition

Three models with standardized output:

| Model                     | Backend      | Entities                       |
| ------------------------- | ------------ | ------------------------------ |
| **CAMeL Tools**     | AraBERT      | PERS, LOC, ORG, MISC           |
| **Hatmimoha**       | BERT         | PERSON, LOCATION, ORGANIZATION |
| **Stanford Stanza** | Multilingual | PER, LOC, ORG                  |

**Output Format**: Unified dictionary with text and label

### Text Summarization

**Extractive Methods** (Sumy):

- LexRank: Graph-based ranking
- LSA: Latent Semantic Analysis
- TextRank: PageRank adaptation

**Abstractive Methods** (Neural):

- **AraBART**: Arabic-specific BART model
- **mT5-XLSum**: Multilingual mT5 fine-tuned on XLSum

### Sentiment Analysis

- **Model**: CAMeL Tools Sentiment Analyzer
- **Labels**: Positive, Negative, Neutral
- **Evaluation**: Accuracy on reference labels

### Topic Modeling

- **Algorithm**: Latent Dirichlet Allocation (LDA)
- **Topics**: 3 topics (configurable)
- **Metrics**: Coherence score (C_V measure)
- **Output**: Top 5 words per topic

---

## Architecture

### Class Hierarchy

```
UltimatePipeline (Main Orchestrator)
â”œâ”€â”€ ArabicPreprocessor (Text Normalization)
â”œâ”€â”€ ArabicNER (Named Entity Recognition)
â”‚   â”œâ”€â”€ CAMeL NER
â”‚   â”œâ”€â”€ Hatmimoha NER
â”‚   â””â”€â”€ Stanza NER
â”œâ”€â”€ ArabicSummarizer (Text Summarization)
â”‚   â”œâ”€â”€ Extractive Models (Sumy)
â”‚   â”œâ”€â”€ Abstractive Models (Neural)
â”‚   â””â”€â”€ Preprocessing Pipeline
â”œâ”€â”€ TopicModeler (Topic Extraction)
â”‚   â””â”€â”€ LDA with Gensim
â”œâ”€â”€ SentimentAnalyzer (CAMeL Tools)
â””â”€â”€ EvaluationMetrics (All Metrics)
    â”œâ”€â”€ ROUGE-1 Scorer
    â”œâ”€â”€ NER Metrics
    â””â”€â”€ Text Normalization
```

### Data Flow

```
Raw Arabic Text
    â†“
[Preprocessing & Normalization]
    â†“
[Parallel Processing]
â”œâ”€â†’ NER Extraction (3 models)
â”œâ”€â†’ Summarization (5 methods)
â”œâ”€â†’ Sentiment Analysis
â””â”€â†’ Topic Modeling
    â†“
[Evaluation & Metrics]
â”œâ”€â†’ ROUGE Scores
â”œâ”€â†’ NER F1, Precision, Recall
â”œâ”€â†’ Sentiment Accuracy
â””â”€â†’ Coherence Score
    â†“
Benchmark Results
```

### Custom Dataset

```python
from Ar-SUM_NER import UltimatePipeline

# Prepare your dataset
my_data = [
    {
        'text': 'Ø£Ø¹Ù„Ù†Øª Ø§Ù„Ø´Ø±ÙƒØ©...',  # Your Arabic text
        'reference_summary': 'Ù…Ù„Ø®Øµ Ù…Ø±Ø¬Ø¹ÙŠ...',
        'entities': [{'text': 'Ø§Ù„Ø´Ø±ÙƒØ©', 'label': 'ORG'}, ...],
        'sentiment': 'positive'
    },
    ...
]

# Run pipeline
pipeline = UltimatePipeline()
pipeline.run(my_data)
```

### Individual Components

```python
from Ar-SUM_NER import ArabicPreprocessor, ArabicNER, ArabicSummarizer

# Initialize components
preprocessor = ArabicPreprocessor()
ner = ArabicNER()
summarizer = ArabicSummarizer(preprocessor)

# Use individually
text = "Ø£Ø¹Ù„Ù†Øª Ø´Ø±ÙƒØ© Ø£Ø±Ø§Ù…ÙƒÙˆ..."
tokens = preprocessor.preprocess(text)
entities = ner.extract_all(text)
summaries = summarizer.summarize(text)
```

---

## Sample Output

### Pipeline Execution Output

```
======================================================================
ARABIC NLP PIPELINE: BENCHMARK EDITION
======================================================================
  Loading CAMeL Morphology...
  Loading NER Models...
  Loading Summarization Models...
  Topic Modeling: Gensim

======================================================================
ğŸ“„ DETAILED ANALYSIS (LARGE DOCS)
======================================================================

Document 1 (317 words)
ğŸ“ Summarization:
   [AraBART]: ÙˆÙ‚Ø¹Øª Ø´Ø±ÙƒØ© Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ø¹ ØªÙˆØªØ§Ù„ ÙˆØ´Ù„...

ğŸ·ï¸ NER (CAMeL):
   Entities found: Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©, Ø£Ù…ÙŠÙ† Ø§Ù„Ù†Ø§ØµØ±, Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†, ØªÙˆØªØ§Ù„ Ø¥Ù†Ø±Ø¬ÙŠØ², Ø´Ù„...

ğŸ˜Š Sentiment:
   True: mixed | Pred: positive

Document 2 (253 words)
Summarization:
AraBART]: Ø§Ø®ØªØªÙ…Øª Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø¯Ø¹ÙˆØ§Øª Ù„Ù„ØªØ¶Ø§Ù…Ù† ÙÙŠ Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª...

NER (CAMeL):
   Entities found: Ø¹Ù…Ø§Ù†, Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø«Ø§Ù†ÙŠ, Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù†, Ø§Ù„Ø³ÙŠØ³ÙŠ...

Sentiment:
   True: neutral | Pred: neutral

Document 3 (292 words)
Summarization:
   [AraBART]: Ø£Ø·Ù„Ù‚Øª Ø¬Ø§Ù…Ø¹Ø© ÙƒØ§ÙˆØ³Øª Ù…Ø¨Ø§Ø¯Ø±Ø© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ Ø¬ÙˆØ¬Ù„...

NER (CAMeL):
   Entities found: ÙƒØ§ÙˆØ³Øª, Ø¬ÙˆØ¬Ù„, Ù…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª, Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ù…Ù„Ùƒ ÙÙŠØµÙ„...

Sentiment:
   True: positive | Pred: positive

======================================================================
FINAL BENCHMARK SCORES
======================================================================

SUMMARIZATION (ROUGE-1)
  mT5-XLSum        : 0.5234
  AraBART          : 0.4892
  Sumy-TextRank    : 0.4156
  Sumy-LexRank     : 0.4023
  Sumy-LSA         : 0.3845

NER (F1 Score)
  CAMeL            : 0.8234
  Hatmimoha        : 0.7856
  Stanza           : 0.7123

SENTIMENT ACCURACY: 0.89

TOPIC COHERENCE: 0.6234
```

---

## Performance Metrics

### Evaluation Metrics Used

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

```
ROUGE-1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

Where:
  Precision = Overlap / Hypothesis Length
  Recall = Overlap / Reference Length
  Overlap = Matching n-grams
```

**Interpretation**:

- 0.0-0.3: Poor
- 0.3-0.5: Fair
- 0.5-0.7: Good
- 0.7+: Excellent

#### NER Metrics

```
Precision = Correct Entities / Total Predicted
Recall = Correct Entities / Total Reference
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**Partial Matching**: Entities with overlapping text are counted as partial matches.

#### Coherence Score (Topic Modeling)

```
Coherence (C_V) âˆˆ [0, 1]

- 0.0-0.3: Topics not well-separated
- 0.3-0.6: Moderate coherence
- 0.6+: Highly coherent topics
```

#### Sentiment Accuracy

```
Accuracy = Correct Predictions / Total Predictions
```

## Models & Components

### Model Sizes & Download Times

| Component     | Model          | Size   | Download Time |
| ------------- | -------------- | ------ | ------------- |
| NER           | AraBERT        | 541 MB | ~5 min        |
| Sentiment     | ARABERT        | 541 MB | ~5 min        |
| Morphology    | CALIMA-MSA-r13 | 40 MB  | <1 min        |
| Summarization | mT5-XLSum      | 2.8 GB | ~15 min       |
| Summarization | AraBART        | 1.8 GB | ~10 min       |

### Model Cards

#### CAMeL Tools NER (AraBERT)

- **Type**: Token Classification (BERT)
- **Training**: Arabic Wikipedia + News
- **Entities**: PERSON, LOCATION, ORGANIZATION, MISCELLANEOUS
- **Input**: Raw or preprocessed Arabic text
- **Output**: Token-level BIO tags

#### Hatmimoha NER

- **Type**: Token Classification (BERT)
- **Base Model**: BERT-base-arabic
- **Training**: Arabic Wikipedia
- **Entities**: PERSON, LOCATION, ORGANIZATION
- **Aggregation Strategy**: Simple (takes first token)

#### Stanford Stanza

- **Type**: Multilingual NLP Pipeline
- **Processors**: Tokenization, NER
- **Language**: Arabic (ar)
- **Architecture**: BiLSTM + Attention

#### AraBART

- **Type**: Sequence-to-Sequence (Transformer)
- **Base**: mBART (multilingual BART)
- **Fine-tuning**: Arabic Summarization datasets
- **Input**: Raw Arabic text
- **Output**: Summary text

#### mT5-XLSum

- **Type**: Sequence-to-Sequence (T5)
- **Multilingual**: 101 languages
- **Fine-tuning**: XLSum (cross-lingual)
- **Max Input**: 512 tokens
- **Max Output**: 150 tokens
