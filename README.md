# ğŸš€ Arabic NLP Pipeline: Benchmark Edition

This project provides a comprehensive Benchmarking Suite for Arabic Natural Language Processing (NLP). It aims to compare the performance of specialized traditional tools (such as CAMeL Tools and AraBERT) against modern Large Language Models (LLMs) like Gemma 3 and Qwen 3 via "Ollama" across tasks including Entity Extraction, Summarization, Sentiment Analysis, and Topic Modeling.

## ğŸ“‹ Key Features

- **Summarization**: Comparison between Extractive methods (LexRank, LSA) and Abstractive models (mT5, AraBART) vs. LLMs.
- **Named Entity Recognition (NER)**: Accuracy evaluation of CAMeL Tools, Stanza, and Hatmimoha against Ollama-hosted models.
- **Sentiment Analysis**: Measuring prediction accuracy for emotional states in Arabic text.
- **Topic Modeling**: Utilizing LDA (Gensim) compared to semantic thematic analysis from Large Language Models.
- **Performance Metrics**: Calculation of Accuracy, F1 Score, ROUGE scores, and execution Runtime.

## ğŸ› ï¸ Requirements

The following libraries are required to run the suite:

```bash
pip install camel-tools transformers torch scikit-learn gensim sumy nltk ollama
```

Additionally, Ollama must be installed with the following models pulled:

```bash
ollama pull gemma3:4b
ollama pull qwen2.5:3b  # Used as a surrogate for Qwen3 in testing
```

## ğŸ“Š Benchmark Results

Based on the latest execution of Ar-SUM_NER.py across three test documents (e.g., Aramco profits, Arab Summit, AI initiatives), the results are as follows:

### 1. Summarization

| Model | Accuracy (ROUGE-1) | Avg. Time (sec) | Sample Output Snippet |
|-------|-------------------|-----------------|----------------------|
| mT5-XLSum | 0.2083 (Best) | 26.11 | "Ø£Ø¹Ù„Ù†Øª Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø¹Ù† Ø£Ø±Ø¨Ø§Ø­..." |
| Sumy (LexRank/LSA) | 0.1828 | 26.11 | (Extracted Sentences) |
| AraBART | 0.1798 | 26.11 | "ØªØ´Ù‡Ø¯ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø·ÙØ±Ø© ØªÙ‚Ù†ÙŠØ©..." |
| LangExtract (Gemma 3) | 0.1619 | 26.60 | "Ù…Ù„Ø®Øµ Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ù„Ø´Ø±ÙƒØ©..." |
| LLM-Only | 0.1193 | 21.22 | "Ø¥Ù„ÙŠÙƒ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ..." |

### 2. Named Entity Recognition (NER)

| Model | F1 Score | Avg. Time (sec) | Sample Entities |
|-------|----------|-----------------|-----------------|
| CAMeL (AraBERT) | 0.8413 (Highest) | 7.23 | Ø§Ù„Ø¸Ù‡Ø±Ø§Ù† (LOC), Ø£Ù…ÙŠÙ† Ø§Ù„Ù†Ø§ØµØ± (PERS) |
| LangExtract / LLM | 0.7417 | 20.83 | Ø£Ø±Ø§Ù…ÙƒÙˆ (ORG), Ø§Ù„Ø£Ø±Ø¯Ù† (LOC) |
| Stanza | 0.7143 | 7.23 | Ø¹Ù…Ø§Ù† (LOC), Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù† (PERS) |
| Hatmimoha | 0.6088 | 7.23 | Ø³ÙˆØ±ÙŠØ§ (LOC) |

### 3. Sentiment Analysis

- Specialized Models Accuracy: 0.67 (e.g., correctly identifying Ø¥ÙŠØ¬Ø§Ø¨ÙŠ - Positive)
- LLM-Only Accuracy: 0.33
- Qwen3-Only Accuracy: 0.33

## ğŸ“ Full Arabic Output Samples (Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©)

Below are the detailed Arabic outputs generated during the benchmark execution:

### Summarization (Ø§Ù„ØªÙ„Ø®ÙŠØµ)

- [LangExtract - Gemma 3]: "Ø£Ø¹Ù„Ù†Øª Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø¹Ù† Ø£Ø±Ø¨Ø§Ø­ Ù‚ÙŠØ§Ø³ÙŠØ© Ø¨Ù„ØºØª 115 Ù…Ù„ÙŠØ§Ø± Ø±ÙŠØ§Ù„ØŒ Ù…Ø¯ÙÙˆØ¹Ø© Ø¨Ø§Ø±ØªÙØ§Ø¹ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù†ÙØ· ÙˆØ²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ù…Ù…Ø§ ÙŠØ¹Ø²Ø² Ù…ÙƒØ§Ù†ØªÙ‡Ø§ ÙƒØ£ÙƒØ¨Ø± Ø´Ø±ÙƒØ© Ø·Ø§Ù‚Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…."
- [AraBART]: "Ø§Ø¹Ù„Ù†Øª Ø´Ø±ÙƒÙ‡ Ø§Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠÙ‡ØŒ Ø¹Ù…Ù„Ø§Ù‚ Ø§Ù„Ù†ÙØ· Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ ÙˆØ§ÙƒØ¨Ø± Ø´Ø±ÙƒÙ‡ Ø·Ø§Ù‚Ù‡ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ù‚ÙŠÙ…Ù‡ Ø§Ù„Ø³ÙˆÙ‚ÙŠÙ‡ØŒ Ø§Ù„ÙŠ ØªØ­Ù‚ÙŠÙ‚ Ø£Ø±Ø¨Ø§Ø­ Ù‚ÙŠØ§Ø³ÙŠØ©."
- [LLM-Only]: "Ø®ÙØµÙ‘ØµÙØª Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø·Ø§Ø±Ø¦Ø© ÙÙŠ Ø§Ù„Ø£Ø±Ø¯Ù† Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…ØªÙØ¬Ø± ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© ÙˆØ³Ø¨Ù„ Ø¯Ø¹Ù… Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¥Ù‚Ù„ÙŠÙ…ÙŠ."

### Named Entities (Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©)

Entities Found: Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© (ORG), Ø£Ù…ÙŠÙ† Ø­Ø³Ù† Ø§Ù„Ù†Ø§ØµØ± (PERS), Ø§Ù„Ø¸Ù‡Ø±Ø§Ù† (LOC), Ø¹Ù…Ø§Ù† (LOC), Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø«Ø§Ù†ÙŠ (PERS), Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ù„Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ© (ORG), Ø¬ÙˆØ¬Ù„ (ORG).

### Topic Analysis (ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹)

- Doc 1 Topics: Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ù„Ø£Ø±Ø§Ù…ÙƒÙˆ (Aramco Financial Results), Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø´Ø±Ø§ÙƒØ© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© (Strategic Partnerships).
- Doc 2 Topics: Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø·Ø§Ø±Ø¦Ø© (Arab Emergency Summit), Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…ØªÙØ¬Ø± ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© (Regional Crisis).
- Doc 3 Topics: Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø© Ø§Ù„ÙˆØ·Ù†ÙŠØ© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (National AI Initiative), Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„Ø·Ø¨ÙŠ (Medical Advancement).

### Sentiment Labels (ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±)

Labels: Ø¥ÙŠØ¬Ø§Ø¨ÙŠ (Positive), Ù…Ø­Ø§ÙŠØ¯ (Neutral), Ù…Ø®ØªÙ„Ø· (Mixed).

## ğŸš€ How to Run

Run the main script to initiate the comparison:

```bash
python arabic_nlp_benchmark.py
```
