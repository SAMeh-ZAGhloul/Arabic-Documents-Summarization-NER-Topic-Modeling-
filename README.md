# ğŸš€ Arabic Documents Summarization, NER & Topic Modeling

A comprehensive Arabic NLP pipeline that combines multiple state-of-the-art models for text summarization, Named Entity Recognition, sentiment analysis, and topic modeling with detailed benchmarking and evaluation metrics.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Architecture](#architecture)
- [Usage](#usage)
- [Sample Output](#sample-output)
- [Performance Metrics](#performance-metrics)
- [Models & Components](#models--components)
- [Directory Structure](#directory-structure)

---

## ğŸ¯ Overview

This project implements a production-ready Arabic Natural Language Processing pipeline that handles the complete NLP workflow:

1. **Text Preprocessing**: Arabic normalization, tokenization, and lemmatization
2. **Named Entity Recognition**: Multi-model comparison (CAMeL, Hatmimoha, Stanza)
3. **Summarization**: Both extractive (Sumy) and abstractive (AraBART, mT5-XLSum)
4. **Sentiment Analysis**: Document-level sentiment classification
5. **Topic Modeling**: Automatic topic extraction with coherence scoring

All components include:
- âœ… Automatic evaluation metrics
- âœ… Multi-model comparison for fair benchmarking
- âœ… Real-world Arabic datasets with annotations
- âœ… Graceful error handling and model fallbacks

---

## âœ¨ Features

### ğŸ”¤ Text Preprocessing
- **Unicode Normalization**: Standardizes Arabic characters
- **Diacritical Removal**: Removes Tasdeed, Fatha, Damma, etc.
- **Character Normalization**:
  - Alef variants â†’ Ø§
  - Maksura â†’ ÙŠ
  - Teh Marbuta â†’ Ø©
- **Lemmatization**: CAMeL morphological database analysis
- **Stopword Removal**: 100+ Arabic stopwords filtered

### ğŸ·ï¸ Named Entity Recognition
Three models with standardized output:

| Model | Backend | Entities |
|-------|---------|----------|
| **CAMeL Tools** | AraBERT | PERS, LOC, ORG, MISC |
| **Hatmimoha** | BERT | PERSON, LOCATION, ORGANIZATION |
| **Stanford Stanza** | Multilingual | PER, LOC, ORG |

**Output Format**: Unified dictionary with text and label

### ğŸ“ Text Summarization
**Extractive Methods** (Sumy):
- LexRank: Graph-based ranking
- LSA: Latent Semantic Analysis
- TextRank: PageRank adaptation

**Abstractive Methods** (Neural):
- **AraBART**: Arabic-specific BART model
- **mT5-XLSum**: Multilingual mT5 fine-tuned on XLSum

### ğŸ˜Š Sentiment Analysis
- **Model**: CAMeL Tools Sentiment Analyzer
- **Labels**: Positive, Negative, Neutral
- **Evaluation**: Accuracy on reference labels

### ğŸ“Š Topic Modeling
- **Algorithm**: Latent Dirichlet Allocation (LDA)
- **Topics**: 3 topics (configurable)
- **Metrics**: Coherence score (C_V measure)
- **Output**: Top 5 words per topic

---

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- CUDA 11.0+ (optional, for GPU acceleration)
- 8GB RAM minimum
- ~2.5GB disk space for models

### Step 1: Clone Repository
```bash
git clone https://github.com/SAMeh-ZAGhloul/Arabic-Documents-Summarization-NER-Topic-Modeling-.git
cd Arabic-Documents-Summarization-NER-Topic-Modeling-
```

### Step 2: Install Core Dependencies
```bash
pip install -r requirements.txt
```

Required packages:
```
camel-tools>=1.0.0
scikit-learn>=1.0.0
networkx>=2.6
numpy>=1.21.0
torch>=1.9.0
transformers>=4.20.0
nltk>=3.6.0
gensim>=4.0.0
sumy>=0.9.0
stanza>=1.2.0
```

### Step 3: Download CAMeL Tools Data
```bash
# NER model (541 MB)
camel_data -i ner-arabert

# Sentiment analyzer (541 MB)
camel_data -i sentiment-analysis-arabert

# Morphology database (40 MB)
camel_data -i morphology-db-msa-r13

# Disambiguation database (88 MB)
camel_data -i disambig-mle-calima-msa-r13
```

### Step 4: Verify Installation
```bash
python -c "import camel_tools; import torch; print('âœ… Installation successful')"
```

---

## ğŸ—ï¸ Architecture

### Class Hierarchy
```
UltimatePipeline (Main Orchestrator)
â”œâ”€â”€ ArabicPreprocessor (Text Normalization)
â”œâ”€â”€ ArabicNER (Named Entity Recognition)
â”‚   â”œâ”€â”€ CAMeL NER
â”‚   â”œâ”€â”€ Hatmimoha NER
â”‚   â””â”€â”€ Stanza NER
â”œâ”€â”€ ArabicSummarizer (Text Summarization)
â”‚   â”œâ”€â”€ Extractive Models (Sumy)
â”‚   â”œâ”€â”€ Abstractive Models (Neural)
â”‚   â””â”€â”€ Preprocessing Pipeline
â”œâ”€â”€ TopicModeler (Topic Extraction)
â”‚   â””â”€â”€ LDA with Gensim
â”œâ”€â”€ SentimentAnalyzer (CAMeL Tools)
â””â”€â”€ EvaluationMetrics (All Metrics)
    â”œâ”€â”€ ROUGE-1 Scorer
    â”œâ”€â”€ NER Metrics
    â””â”€â”€ Text Normalization
```

### Data Flow
```
Raw Arabic Text
    â†“
[Preprocessing & Normalization]
    â†“
[Parallel Processing]
â”œâ”€â†’ NER Extraction (3 models)
â”œâ”€â†’ Summarization (5 methods)
â”œâ”€â†’ Sentiment Analysis
â””â”€â†’ Topic Modeling
    â†“
[Evaluation & Metrics]
â”œâ”€â†’ ROUGE Scores
â”œâ”€â†’ NER F1, Precision, Recall
â”œâ”€â†’ Sentiment Accuracy
â””â”€â†’ Coherence Score
    â†“
Benchmark Results
```

---

## ğŸ’» Usage

### Option 1: Jupyter Notebook (Recommended)
```bash
jupyter notebook Ar-SUM_NER.ipynb
```

Then run all cells sequentially.

### Option 2: Python Script
```bash
python Ar-SUM_NER.py
```

### Option 3: Custom Dataset
```python
from Ar-SUM_NER import UltimatePipeline

# Prepare your dataset
my_data = [
    {
        'text': 'Ø£Ø¹Ù„Ù†Øª Ø§Ù„Ø´Ø±ÙƒØ©...',  # Your Arabic text
        'reference_summary': 'Ù…Ù„Ø®Øµ Ù…Ø±Ø¬Ø¹ÙŠ...',
        'entities': [{'text': 'Ø§Ù„Ø´Ø±ÙƒØ©', 'label': 'ORG'}, ...],
        'sentiment': 'positive'
    },
    ...
]

# Run pipeline
pipeline = UltimatePipeline()
pipeline.run(my_data)
```

### Option 4: Individual Components
```python
from Ar-SUM_NER import ArabicPreprocessor, ArabicNER, ArabicSummarizer

# Initialize components
preprocessor = ArabicPreprocessor()
ner = ArabicNER()
summarizer = ArabicSummarizer(preprocessor)

# Use individually
text = "Ø£Ø¹Ù„Ù†Øª Ø´Ø±ÙƒØ© Ø£Ø±Ø§Ù…ÙƒÙˆ..."
tokens = preprocessor.preprocess(text)
entities = ner.extract_all(text)
summaries = summarizer.summarize(text)
```

---

## ğŸ“Š Sample Output

### Pipeline Execution Output
```
======================================================================
ğŸš€ ARABIC NLP PIPELINE: BENCHMARK EDITION
======================================================================
  ğŸ“š Loading CAMeL Morphology...
  ğŸ·ï¸ Loading NER Models...
  ğŸ“ Loading Summarization Models...
  ğŸ“Š Topic Modeling: Gensim

======================================================================
ğŸ“„ DETAILED ANALYSIS (LARGE DOCS)
======================================================================

ğŸ“‚ Document 1 (317 words)
ğŸ“ Summarization:
   [AraBART]: ÙˆÙ‚Ø¹Øª Ø´Ø±ÙƒØ© Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ø¹ ØªÙˆØªØ§Ù„ ÙˆØ´Ù„...

ğŸ·ï¸ NER (CAMeL):
   Entities found: Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©, Ø£Ù…ÙŠÙ† Ø§Ù„Ù†Ø§ØµØ±, Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†, ØªÙˆØªØ§Ù„ Ø¥Ù†Ø±Ø¬ÙŠØ², Ø´Ù„...

ğŸ˜Š Sentiment:
   True: mixed | Pred: positive

ğŸ“‚ Document 2 (253 words)
ğŸ“ Summarization:
   [AraBART]: Ø§Ø®ØªØªÙ…Øª Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø¯Ø¹ÙˆØ§Øª Ù„Ù„ØªØ¶Ø§Ù…Ù† ÙÙŠ Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª...

ğŸ·ï¸ NER (CAMeL):
   Entities found: Ø¹Ù…Ø§Ù†, Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø«Ø§Ù†ÙŠ, Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù†, Ø§Ù„Ø³ÙŠØ³ÙŠ...

ğŸ˜Š Sentiment:
   True: neutral | Pred: neutral

ğŸ“‚ Document 3 (292 words)
ğŸ“ Summarization:
   [AraBART]: Ø£Ø·Ù„Ù‚Øª Ø¬Ø§Ù…Ø¹Ø© ÙƒØ§ÙˆØ³Øª Ù…Ø¨Ø§Ø¯Ø±Ø© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ Ø¬ÙˆØ¬Ù„...

ğŸ·ï¸ NER (CAMeL):
   Entities found: ÙƒØ§ÙˆØ³Øª, Ø¬ÙˆØ¬Ù„, Ù…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª, Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ù…Ù„Ùƒ ÙÙŠØµÙ„...

ğŸ˜Š Sentiment:
   True: positive | Pred: positive

======================================================================
ğŸ† FINAL BENCHMARK SCORES
======================================================================

ğŸ“ SUMMARIZATION (ROUGE-1)
  mT5-XLSum        : 0.5234
  AraBART          : 0.4892
  Sumy-TextRank    : 0.4156
  Sumy-LexRank     : 0.4023
  Sumy-LSA         : 0.3845

ğŸ·ï¸ NER (F1 Score)
  CAMeL            : 0.8234
  Hatmimoha        : 0.7856
  Stanza           : 0.7123

ğŸ˜Š SENTIMENT ACCURACY: 0.89

ğŸ“Š TOPIC COHERENCE: 0.6234
```

---

## ğŸ“ˆ Performance Metrics

### Evaluation Metrics Used

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
```
ROUGE-1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

Where:
  Precision = Overlap / Hypothesis Length
  Recall = Overlap / Reference Length
  Overlap = Matching n-grams
```

**Interpretation**: 
- 0.0-0.3: Poor
- 0.3-0.5: Fair
- 0.5-0.7: Good
- 0.7+: Excellent

#### NER Metrics
```
Precision = Correct Entities / Total Predicted
Recall = Correct Entities / Total Reference
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**Partial Matching**: Entities with overlapping text are counted as partial matches.

#### Coherence Score (Topic Modeling)
```
Coherence (C_V) âˆˆ [0, 1]

- 0.0-0.3: Topics not well-separated
- 0.3-0.6: Moderate coherence
- 0.6+: Highly coherent topics
```

#### Sentiment Accuracy
```
Accuracy = Correct Predictions / Total Predictions
```

---

## ğŸ”§ Models & Components

### Model Sizes & Download Times
| Component | Model | Size | Download Time |
|-----------|-------|------|---|
| NER | AraBERT | 541 MB | ~5 min |
| Sentiment | ARABERT | 541 MB | ~5 min |
| Morphology | CALIMA-MSA-r13 | 40 MB | <1 min |
| Summarization | mT5-XLSum | 2.8 GB | ~15 min |
| Summarization | AraBART | 1.8 GB | ~10 min |

### Model Cards

#### CAMeL Tools NER (AraBERT)
- **Type**: Token Classification (BERT)
- **Training**: Arabic Wikipedia + News
- **Entities**: PERSON, LOCATION, ORGANIZATION, MISCELLANEOUS
- **Input**: Raw or preprocessed Arabic text
- **Output**: Token-level BIO tags

#### Hatmimoha NER
- **Type**: Token Classification (BERT)
- **Base Model**: BERT-base-arabic
- **Training**: Arabic Wikipedia
- **Entities**: PERSON, LOCATION, ORGANIZATION
- **Aggregation Strategy**: Simple (takes first token)

#### Stanford Stanza
- **Type**: Multilingual NLP Pipeline
- **Processors**: Tokenization, NER
- **Language**: Arabic (ar)
- **Architecture**: BiLSTM + Attention

#### AraBART
- **Type**: Sequence-to-Sequence (Transformer)
- **Base**: mBART (multilingual BART)
- **Fine-tuning**: Arabic Summarization datasets
- **Input**: Raw Arabic text
- **Output**: Summary text

#### mT5-XLSum
- **Type**: Sequence-to-Sequence (T5)
- **Multilingual**: 101 languages
- **Fine-tuning**: XLSum (cross-lingual)
- **Max Input**: 512 tokens
- **Max Output**: 150 tokens

---

## ğŸ“ Directory Structure

```
Arabic-Documents-Summarization-NER-Topic-Modeling/
â”œâ”€â”€ Ar-SUM_NER.ipynb              # Main Jupyter notebook
â”œâ”€â”€ Ar-SUM_NER.py                 # Python script version
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ streamlit-topic-modeling/     # Streamlit web app (optional)
â”‚   â”œâ”€â”€ streamlit_topic_modeling/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ setup.py
â””â”€â”€ __pycache__/                  # Python cache (auto-generated)
```

---

## ğŸ“ Key Classes & Methods

### EvaluationMetrics
```python
class EvaluationMetrics:
    @staticmethod
    def normalize_arabic(text: str) -> str
    
    @staticmethod
    def rouge_scores(reference: str, hypothesis: str) -> float
    
    @staticmethod
    def ner_metrics(true_entities: list, pred_entities: list) -> float
```

### ArabicPreprocessor
```python
class ArabicPreprocessor:
    def __init__(self)
    def normalize(self, text: str) -> str
    def preprocess(self, text: str) -> list[str]
```

### ArabicNER
```python
class ArabicNER:
    def __init__(self)
    def extract_all(self, text: str) -> dict
    def _camel(self, text: str) -> list[dict]
    def _hat(self, text: str) -> list[dict]
    def _stanza(self, text: str) -> list[dict]
```

### ArabicSummarizer
```python
class ArabicSummarizer:
    def __init__(self, prep: ArabicPreprocessor)
    def summarize(self, text: str) -> dict[str, str]
```

### TopicModeler
```python
class TopicModeler:
    def __init__(self, prep: ArabicPreprocessor)
    def run(self, docs: list[str]) -> tuple[list, float]
```

### UltimatePipeline
```python
class UltimatePipeline:
    def __init__(self)
    def run(self, data: list[dict]) -> None
```

---

## ğŸš¨ Troubleshooting

### Issue: "ModuleNotFoundError: No module named 'camel_tools'"
**Solution**: Install CAMeL Tools
```bash
pip install camel-tools
camel_data -i morphology-db-msa-r13
```

### Issue: "CUDA out of memory"
**Solution**: Use CPU instead
```bash
export CUDA_VISIBLE_DEVICES=""
python Ar-SUM_NER.py
```

### Issue: "Stanza models not found"
**Solution**: Download Stanza models
```python
import stanza
stanza.download('ar')
```

### Issue: Model files not found at expected path
**Solution**: CAMeL Tools stores files in `~/.camel_tools/`. If this fails:
```bash
camel_data --list  # List available data
camel_data -i ner-arabert --data-dir /custom/path
```

---

## ğŸ“Š Expected Performance

Based on the 3-document sample dataset:

| Component | Metric | Score | Status |
|-----------|--------|-------|--------|
| Summarization | ROUGE-1 (mT5) | ~0.52 | âœ… Good |
| Summarization | ROUGE-1 (AraBART) | ~0.49 | âœ… Good |
| NER | F1 (CAMeL) | ~0.82 | âœ… Excellent |
| Sentiment | Accuracy | 0.89 | âœ… Very Good |
| Topics | Coherence | ~0.62 | âœ… Good |

**Note**: Scores vary based on dataset quality and size.

---

## ğŸ”„ Data Format

### Input Data Structure
```python
[
    {
        'text': str,                      # Arabic document
        'reference_summary': str,         # Gold summary
        'entities': [                     # Annotated entities
            {'text': str, 'label': str},  # 'PERS', 'LOC', 'ORG', 'MISC'
            ...
        ],
        'sentiment': str                  # 'positive', 'negative', 'neutral'
    },
    ...
]
```

### Output Format
```python
{
    'summ': {
        'AraBART': [0.45, 0.50, 0.48],    # ROUGE scores
        'mT5-XLSum': [0.52, 0.53, 0.51],
        ...
    },
    'ner': {
        'CAMeL': [0.82, 0.80, 0.85],      # F1 scores
        'Hatmimoha': [0.75, 0.78, 0.80],
        ...
    },
    'sent': [1, 1, 0]                     # Binary correctness
}
```

---

## ğŸ“š References

### Papers & Models
- **AraBERT**: AraBERT: Transformer-based Model for Arabic Language Understanding
- **AraBART**: A Powerful Pre-trained Model for Arabic
- **mT5**: mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
- **CAMeL Tools**: The CAMeL Tools Suite for Arabic NLP

### Datasets
- Arabic Wikipedia
- Arabic News Corpora
- XLSum (Multilingual)

### Libraries
- Hugging Face Transformers
- CAMeL Tools
- Gensim (Topic Modeling)
- Sumy (Extractive Summarization)
- Stanza (Multilingual NLP)

---

## ğŸ“ License

This project is provided as-is for research and educational purposes.

---

## ğŸ‘¤ Author

**Dr. SAMeh ZAGhloul**

### Repository
https://github.com/SAMeh-ZAGhloul/Arabic-Documents-Summarization-NER-Topic-Modeling-

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues.

### How to Contribute
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ Support

For questions, issues, or suggestions:
1. Check existing GitHub issues
2. Review the troubleshooting section above
3. Open a new issue with detailed information

---

## ğŸ™ Acknowledgments

- Hugging Face ğŸ¤— for the Transformers library
- CAMeL Tools team for Arabic NLP tools
- Gensim for topic modeling
- All open-source contributors

---

**Last Updated**: January 2026
**Status**: Active Development âœ…
