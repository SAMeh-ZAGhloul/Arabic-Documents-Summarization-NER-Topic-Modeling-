{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f0685f",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# Arabic Documents Summarization, NER & Topic Modeling\n",
    "\n",
    "### Install CAMeL Tools\n",
    "pip install camel-tools scikit-learn networkx numpy\n",
    "\n",
    "### Download required models\n",
    "camel_data -i ner-arabert                    \n",
    "camel_data -i sentiment-analysis-arabert     \n",
    "camel_data -i morphology-db-msa-r13          \n",
    "camel_data -i disambig-mle-calima-msa-r13    \n",
    "\n",
    "## Components:\n",
    "- Preprocessing & Lemmatization (CAMeL Tools)\n",
    "- Named Entity Recognition (AraBERT)\n",
    "- Sentiment Analysis (CAMeL Tools)\n",
    "- Topic Modeling (LDA)\n",
    "- Extractive Summarization (TextRank)\n",
    "- Abstractive Summarization (mT5 & AraT5 & AraBART)\n",
    "- Accuracy benchmarks for all components\n",
    "- ROUGE scores for summarization\n",
    "- F1/Precision/Recall for NER\n",
    "- Accuracy for Sentiment\n",
    "- Coherence for Topic Modeling\n",
    "\n",
    "### Summarization:\n",
    "  1. Sumy-LexRank, TextRank, LSA\n",
    "  2. TF-IDF Baseline\n",
    "  3. mT5-XLSum (Abstractive)\n",
    "  4. AraBART (Abstractive)\n",
    "  5. LangExtract (Google's Multilingual Model)\n",
    "\n",
    "### NER Comparison:\n",
    "  1. CAMeL Tools (AraBERT)\n",
    "  2. Stanford Stanza\n",
    "  3. Hatmimoha (BERT)\n",
    "  4. LangExtract (Google's Multilingual Model)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7da495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "# =============================================\n",
    "# ğŸ› ï¸ SETUP & IMPORTS\n",
    "# =============================================\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"â¬‡ï¸ Downloading NLTK data...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline as hf_pipeline\n",
    ")\n",
    "\n",
    "# CAMeL Tools\n",
    "from camel_tools.utils.normalize import normalize_unicode, normalize_alef_ar, normalize_alef_maksura_ar, normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "\n",
    "# Optional Libraries\n",
    "try:\n",
    "    import sumy\n",
    "    from sumy.parsers.plaintext import PlaintextParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer\n",
    "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "    from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "    from sumy.summarizers.lsa import LsaSummarizer\n",
    "    SUMY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SUMY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "    from gensim.models import LdaModel\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "    GENSIM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GENSIM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import stanza\n",
    "    STANZA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    STANZA_AVAILABLE = False\n",
    "\n",
    "# LangExtract\n",
    "try:\n",
    "    import ollama\n",
    "    LANGEXTRACT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LANGEXTRACT_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE_ID = 0 if torch.cuda.is_available() else -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb85cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 1. METRICS\n",
    "# =============================================\n",
    "class EvaluationMetrics:\n",
    "    @staticmethod\n",
    "    def normalize_arabic(text):\n",
    "        text = normalize_unicode(text)\n",
    "        text = dediac_ar(text)\n",
    "        text = normalize_alef_ar(text)\n",
    "        text = normalize_alef_maksura_ar(text)\n",
    "        text = normalize_teh_marbuta_ar(text)\n",
    "        return re.sub(r'\\bØ§Ù„', '', text).lower().strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def rouge_scores(reference, hypothesis):\n",
    "        def tokenize(text):\n",
    "            text = EvaluationMetrics.normalize_arabic(text)\n",
    "            return [t for t in simple_word_tokenize(text) if len(t) > 1 and not t.isdigit()]\n",
    "        \n",
    "        ref_tokens = tokenize(reference)\n",
    "        hyp_tokens = tokenize(hypothesis)\n",
    "        if not ref_tokens or not hyp_tokens: return 0.0\n",
    "        \n",
    "        overlap = sum((Counter(ref_tokens) & Counter(hyp_tokens)).values())\n",
    "        p = overlap / len(hyp_tokens)\n",
    "        r = overlap / len(ref_tokens)\n",
    "        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def ner_metrics(true_entities, pred_entities):\n",
    "        def norm(e): return EvaluationMetrics.normalize_arabic(e['text'])\n",
    "        true_set = set((norm(e), e['label']) for e in true_entities)\n",
    "        partial_tp = 0\n",
    "        for p in pred_entities:\n",
    "            p_norm = norm(p)\n",
    "            for t in true_entities:\n",
    "                t_norm = norm(t)\n",
    "                if p['label'] == t['label'] and (p_norm in t_norm or t_norm in p_norm):\n",
    "                    partial_tp += 1\n",
    "                    break\n",
    "        prec = partial_tp / len(pred_entities) if pred_entities else 0\n",
    "        rec = partial_tp / len(true_entities) if true_entities else 0\n",
    "        return 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "\n",
    "class ArabicPreprocessor:\n",
    "    STOPWORDS = set(\"ÙÙŠ Ù…Ù† Ø¥Ù„Ù‰ Ø¹Ù„Ù‰ Ø£Ù† Ù‡Ø°Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ø°ÙŠ Ø§Ù„ØªÙŠ Ù„ÙƒÙ† ÙƒØ§Ù† Ø¨Ù‡Ø§ Ù‡Ù… Ø¨Ø£Ù† Ù‡Ù†Ø§Ùƒ Ø¹Ù† Ø­ÙŠØ« Ùˆ Ù„Ø§ Ø§Ù„ Ø¨ Ù„ Ùƒ Ù… Ù† Ù‡Ù†Ø§ Ù„Ø°Ø§ Ù„Ø£Ù† Ø­ØªÙ‰ ÙˆÙ…Ø¹ Ø¯ÙˆÙ† Ø£Ùˆ ÙˆÙ…Ø§ ÙƒÙ„ Ø¨Ø¹Ø¯ Ù‚Ø¨Ù„ Ø¹Ù†Ø¯ Ø¨ÙŠÙ† ÙƒÙ…Ø§ Ø£ÙŠØ¶Ø§ Ø«Ù… Ù„Ù… Ù„Ù† Ø¥Ø°Ø§ ÙƒÙŠÙ Ù…Ø§ Ù‡Ù„ Ø£ÙŠ Ù‡Ùˆ Ù‡ÙŠ Ù†Ø­Ù† Ø£Ù†Ø§ Ø£Ù†Øª ÙŠÙƒÙˆÙ† ØªÙƒÙˆÙ† ÙƒØ§Ù†Øª Ø¹Ù„ÙŠÙ‡ Ø¥Ù„ÙŠÙ‡ Ù…Ù†Ù‡ ÙÙŠÙ‡ Ø¨Ù‡ Ø°Ù„Ùƒ ØªÙ„Ùƒ Ù‡Ø¤Ù„Ø§Ø¡ Ø£ÙˆÙ„Ø¦Ùƒ Ø¹Ø§Ù… Ø¨Ø¹Ø¶ Ø¬Ù…ÙŠØ¹ Ø£ÙƒØ«Ø± Ù…Ø¹Ø¸Ù… ØºÙŠØ± Ø®Ù„Ø§Ù„ Ø¶Ù…Ù† Ù†Ø­Ùˆ Ø­ÙˆÙ„ Ù‚Ø¯ Ù‚Ø§Ù„ ÙŠÙ‚ÙˆÙ„ ÙƒØ§Ù†ÙˆØ§ ÙˆÙƒØ§Ù† Ù‚Ø§Ù„Øª ÙŠÙˆÙ… ÙˆÙ‚Ø¯ ÙˆÙ„Ø§ ÙˆÙ„Ù… ÙˆÙ…Ù† ÙˆÙ‡Ùˆ ÙˆÙ‡ÙŠ ÙˆÙ„ÙƒÙ† ÙØ¥Ù† Ø¥Ù„Ø§ Ø£Ù…Ø§\".split())\n",
    "    def __init__(self):\n",
    "        print(\"  ğŸ“š Loading CAMeL Morphology...\")\n",
    "        self.db = MorphologyDB.builtin_db('calima-msa-r13')\n",
    "        self.analyzer = Analyzer(self.db)\n",
    "    def normalize(self, text):\n",
    "        return normalize_teh_marbuta_ar(normalize_alef_maksura_ar(normalize_alef_ar(dediac_ar(normalize_unicode(text)))))\n",
    "    def preprocess(self, text):\n",
    "        text = self.normalize(text)\n",
    "        tokens = simple_word_tokenize(text)\n",
    "        lemmas = []\n",
    "        for t in tokens:\n",
    "            if len(t) < 2 or t in self.STOPWORDS: continue\n",
    "            analyses = self.analyzer.analyze(t)\n",
    "            lemma = analyses[0].get('lex', t) if analyses else t\n",
    "            lemmas.append(re.sub(r'_\\d+$', '', lemma))\n",
    "        return [dediac_ar(l) for l in lemmas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965bca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 2. MODELS (NER, Summ, Topics)\n",
    "# =============================================\n",
    "class ArabicNER:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        print(\"  ğŸ·ï¸ Loading NER Models...\")\n",
    "        # CAMeL\n",
    "        try:\n",
    "            path = \"/Users/user/.camel_tools/data/ner/arabert\"\n",
    "            self.c_tok = AutoTokenizer.from_pretrained(path)\n",
    "            self.c_mod = AutoModelForTokenClassification.from_pretrained(path)\n",
    "            self.c_lbl = {0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PERS', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PERS', 8: 'O'}\n",
    "            self.models['CAMeL'] = True\n",
    "        except: pass\n",
    "        # Hatmimoha\n",
    "        try:\n",
    "            self.h_pipe = hf_pipeline(\"ner\", model=\"hatmimoha/arabic-ner\", aggregation_strategy=\"simple\", device=DEVICE_ID)\n",
    "            self.models['Hatmimoha'] = True\n",
    "        except: pass\n",
    "        # Stanza\n",
    "        try:\n",
    "            if STANZA_AVAILABLE:\n",
    "                stanza.download('ar', processors='tokenize,ner', verbose=False)\n",
    "                self.s_nlp = stanza.Pipeline('ar', processors='tokenize,ner', verbose=False)\n",
    "                self.models['Stanza'] = True\n",
    "        except: pass\n",
    "\n",
    "    def extract_all(self, text):\n",
    "        res = {}\n",
    "        if 'CAMeL' in self.models: res['CAMeL'] = self._camel(text)\n",
    "        if 'Hatmimoha' in self.models: res['Hatmimoha'] = self._hat(text)\n",
    "        if 'Stanza' in self.models: res['Stanza'] = self._stanza(text)\n",
    "        return res\n",
    "\n",
    "    def _camel(self, text):\n",
    "        ents = []\n",
    "        inp = self.c_tok(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad(): preds = torch.argmax(self.c_mod(**inp).logits, dim=2)[0]\n",
    "        toks = self.c_tok.convert_ids_to_tokens(inp['input_ids'][0])\n",
    "        lbls = [self.c_lbl[p.item()] for p in preds]\n",
    "        curr_t, curr_l = [], None\n",
    "        for t, l in zip(toks, lbls):\n",
    "            if t in ['[CLS]', '[SEP]']: continue\n",
    "            if t.startswith('##'): \n",
    "                if curr_t: curr_t[-1] += t[2:]\n",
    "            elif l.startswith('B-'):\n",
    "                if curr_t: ents.append({'text': ' '.join(curr_t), 'label': curr_l})\n",
    "                curr_t, curr_l = [t], l[2:]\n",
    "            elif l.startswith('I-') and curr_l: curr_t.append(t)\n",
    "            else:\n",
    "                if curr_t: ents.append({'text': ' '.join(curr_t), 'label': curr_l})\n",
    "                curr_t, curr_l = [], None\n",
    "        return ents\n",
    "\n",
    "    def _hat(self, text):\n",
    "        try:\n",
    "            r = self.h_pipe(text)\n",
    "            return [{'text': x['word'], 'label': {'PERSON':'PERS','LOCATION':'LOC','ORGANIZATION':'ORG'}.get(x['entity_group'],'MISC')} for x in r]\n",
    "        except: return []\n",
    "\n",
    "    def _stanza(self, text):\n",
    "        try:\n",
    "            doc = self.s_nlp(text)\n",
    "            return [{'text': e.text, 'label': {'PER':'PERS'}.get(e.type, e.type)} for e in doc.ents]\n",
    "        except: return []\n",
    "\n",
    "class ArabicSummarizer:\n",
    "    def __init__(self, prep):\n",
    "        self.prep = prep\n",
    "        self.sumy = {}\n",
    "        if SUMY_AVAILABLE:\n",
    "            print(\"  ğŸ“ Loading Summarization Models...\")\n",
    "            self.sumy = {'Sumy-LexRank': LexRankSummarizer, 'Sumy-LSA': LsaSummarizer}\n",
    "        self.neural = {}\n",
    "        self._load('AraBART', 'moussaKam/AraBART', 'seq2seq')\n",
    "        self._load('mT5-XLSum', 'csebuetnlp/mT5_multilingual_XLSum', 'pipeline')\n",
    "\n",
    "    def _load(self, name, path, type):\n",
    "        try:\n",
    "            if type == 'pipeline': self.neural[name] = {'mod': hf_pipeline(\"summarization\", model=path, device=DEVICE_ID), 'type': 'pipe'}\n",
    "            else:\n",
    "                tok = AutoTokenizer.from_pretrained(path)\n",
    "                mod = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "                mod.to(DEVICE)\n",
    "                self.neural[name] = {'tok': tok, 'mod': mod, 'type': 'seq'}\n",
    "        except: pass\n",
    "\n",
    "    def summarize(self, text):\n",
    "        res = {}\n",
    "        # Sumy\n",
    "        parser = PlaintextParser.from_string(text, SumyTokenizer(\"english\"))\n",
    "        for n, cls in self.sumy.items():\n",
    "            try: res[n] = ' '.join([str(s) for s in cls()(parser.document, 7)])\n",
    "            except: pass\n",
    "        # Neural\n",
    "        clean = ' '.join(self.prep.normalize(text).split())[:4000]\n",
    "        for n, c in self.neural.items():\n",
    "            try:\n",
    "                if c['type'] == 'pipe': res[n] = c['mod'](clean, max_length=150, min_length=50, truncation=True)[0]['summary_text']\n",
    "                else:\n",
    "                    inp = c['tok'](clean, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "                    out = c['mod'].generate(**inp, max_length=150, min_length=50, num_beams=4)\n",
    "                    res[n] = c['tok'].decode(out[0], skip_special_tokens=True)\n",
    "            except: pass\n",
    "        return res\n",
    "\n",
    "class TopicModeler:\n",
    "    def __init__(self, prep):\n",
    "        self.prep = prep\n",
    "        print(f\"  ğŸ“Š Topic Modeling: {'Gensim' if GENSIM_AVAILABLE else 'N/A'}\")\n",
    "\n",
    "    def run(self, docs):\n",
    "        if not GENSIM_AVAILABLE: return None, 0\n",
    "        texts = [self.prep.preprocess(d) for d in docs]\n",
    "        dic = corpora.Dictionary(texts)\n",
    "        dic.filter_extremes(no_below=1, no_above=0.9)\n",
    "        corpus = [dic.doc2bow(t) for t in texts]\n",
    "        lda = LdaModel(corpus, num_topics=3, id2word=dic, passes=20, random_state=42)\n",
    "        return lda.print_topics(num_words=5), CoherenceModel(model=lda, texts=texts, dictionary=dic, coherence='c_v').get_coherence()\n",
    "\n",
    "# =============================================\n",
    "# 2.5 LANGEXTRACT INTEGRATION\n",
    "# =============================================\n",
    "class LangExtractWrapper:\n",
    "    def __init__(self):\n",
    "        self.available = LANGEXTRACT_AVAILABLE\n",
    "        if self.available:\n",
    "            print(\"  ğŸŒ Loading Ollama with gemma3:4b (LLM-based Multilingual Model)...\")\n",
    "            try:\n",
    "                # Test connection to Ollama\n",
    "                ollama.chat(model='gemma3:4b', messages=[{'role': 'user', 'content': 'test'}], options={'num_predict': 10})\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error connecting to Ollama: {e}\")\n",
    "                self.available = False\n",
    "\n",
    "    def summarize(self, text):\n",
    "        if not self.available: return None\n",
    "        try:\n",
    "            # Prepare a prompt for summarization\n",
    "            prompt = f\"\"\"Please provide a concise summary of the following Arabic text. The summary should be in Arabic and capture the main points:\n",
    "\n",
    "{text[:2000]}\"\"\"  # Limit text length to prevent context overflow\n",
    "            \n",
    "            response = ollama.chat(\n",
    "                model='gemma3:4b',\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                options={'num_predict': 150}  # Limit response length\n",
    "            )\n",
    "            \n",
    "            summary = response['message']['content'].strip()\n",
    "            return summary if summary else None\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama summary error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_entities(self, text):\n",
    "        if not self.available: return []\n",
    "        try:\n",
    "            # Prepare a prompt for NER\n",
    "            prompt = f\"\"\"Extract named entities from the following Arabic text. Return the results in JSON format with 'text' and 'label' fields. Labels should be one of: 'PERS' (person), 'ORG' (organization), 'LOC' (location), 'MISC' (miscellaneous).\n",
    "\n",
    "Example format:\n",
    "[\n",
    "    {{\"text\": \"Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\", \"label\": \"ORG\"}},\n",
    "    {{\"text\": \"Ø£Ù…ÙŠÙ† Ø§Ù„Ù†Ø§ØµØ±\", \"label\": \"PERS\"}}\n",
    "]\n",
    "\n",
    "Text:\n",
    "{text[:2000]}\"\"\"  # Limit text length to prevent context overflow\n",
    "            \n",
    "            response = ollama.chat(\n",
    "                model='gemma3:4b',\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                options={'num_predict': 300}  # Allow more space for entity extraction\n",
    "            )\n",
    "            \n",
    "            result = response['message']['content'].strip()\n",
    "            \n",
    "            # Parse the response to extract entities\n",
    "            entities = []\n",
    "            import json as json_module\n",
    "            import re as re_module\n",
    "            \n",
    "            # Look for JSON-like structure in the response\n",
    "            json_match = re_module.search(r'\\[(.*?)\\]', result, re_module.DOTALL)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    # Attempt to parse the JSON portion\n",
    "                    json_str = '[' + json_match.group(1) + ']'\n",
    "                    # Clean up the JSON string to make it valid\n",
    "                    json_str = re_module.sub(r'\\\\*', '', json_str)  # Remove extra escapes\n",
    "                    entities = json_module.loads(json_str)\n",
    "                except:\n",
    "                    # If JSON parsing fails, try to extract entities with regex\n",
    "                    # Look for patterns that match the expected format\n",
    "                    for line in result.split('\\n'):\n",
    "                        # Match patterns like: {\"text\": \"...\", \"label\": \"...\"}\n",
    "                        matches = re_module.findall(r'\"text\":\\s*\"([^\"]+)\"[^}}}]*\"label\":\\s*\"([^\"]+)\"', line)\n",
    "                        for text_val, label_val in matches:\n",
    "                            entities.append({\"text\": text_val, \"label\": label_val})\n",
    "            else:\n",
    "                # If no JSON format found, try to extract using regex patterns\n",
    "                # Look for patterns in the response\n",
    "                lines = result.split('\\n')\n",
    "                for line in lines:\n",
    "                    # Look for patterns that might contain entity information\n",
    "                    if 'text' in line.lower() and 'label' in line.lower():\n",
    "                        # Extract using regex\n",
    "                        text_match = re_module.search(r'\"text\":\\s*\"([^\"]+)\"', line)\n",
    "                        label_match = re_module.search(r'\"label\":\\s*\"([^\"]+)\"', line)\n",
    "                        if text_match and label_match:\n",
    "                            entities.append({\n",
    "                                \"text\": text_match.group(1),\n",
    "                                \"label\": label_match.group(1)\n",
    "                            })\n",
    "            \n",
    "            return entities\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama NER error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_topics(self, text):\n",
    "        if not self.available: return []\n",
    "        try:\n",
    "            # Prepare a prompt for topic extraction\n",
    "            prompt = f\"\"\"Identify the main topics discussed in the following Arabic text. Return a list of 3-5 key topics/phrases that represent the main subjects.\n",
    "\n",
    "Text:\n",
    "{text[:2000]}\"\"\"\n",
    "            \n",
    "            response = ollama.chat(\n",
    "                model='gemma3:4b',\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                options={'num_predict': 100}\n",
    "            )\n",
    "            \n",
    "            result = response['message']['content'].strip()\n",
    "            \n",
    "            # Extract topics from the response\n",
    "            topics = []\n",
    "            import re as re_module\n",
    "            for line in result.split('\\n'):\n",
    "                # Remove numbering or bullet points\n",
    "                cleaned_line = re_module.sub(r'^[\\d\\-\\*\\)\\.]+\\s*', '', line).strip()\n",
    "                if cleaned_line and len(cleaned_line) > 3:  # Meaningful topic\n",
    "                    topics.append(cleaned_line)\n",
    "            \n",
    "            # Limit to top 3 topics\n",
    "            return topics[:3]\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama topic extraction error: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92d9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 3. PIPELINE EXECUTION\n",
    "# =============================================\n",
    "class UltimatePipeline:\n",
    "    def __init__(self):\n",
    "        print(\"=\"*70)\n",
    "        print(\"ğŸš€ ARABIC NLP PIPELINE: BENCHMARK EDITION\")\n",
    "        print(\"=\"*70)\n",
    "        self.prep = ArabicPreprocessor()\n",
    "        self.ner = ArabicNER()\n",
    "        self.summ = ArabicSummarizer(self.prep)\n",
    "        self.topics = TopicModeler(self.prep)\n",
    "        self.metrics = EvaluationMetrics()\n",
    "        self.sentiment = SentimentAnalyzer.pretrained()\n",
    "        self.lang_extract = LangExtractWrapper()\n",
    "\n",
    "    def run(self, data):\n",
    "        scores = {'summ': {}, 'ner': {}, 'sent': []}\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“„ DETAILED ANALYSIS (LARGE DOCS)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for i, d in enumerate(data):\n",
    "            text = d['text']\n",
    "            print(f\"\\nğŸ“‚ Document {i+1} ({len(text.split())} words)\")\n",
    "            \n",
    "            # 1. Summarization\n",
    "            print(\"ğŸ“ Summarization:\")\n",
    "            sums = self.summ.summarize(text)\n",
    "            \n",
    "            # Add LangExtract summarization\n",
    "            if self.lang_extract.available:\n",
    "                le_summary = self.lang_extract.summarize(text)\n",
    "                if le_summary:\n",
    "                    sums['LangExtract'] = le_summary\n",
    "                    r1 = self.metrics.rouge_scores(d['reference_summary'], le_summary)\n",
    "                    scores['summ'].setdefault('LangExtract', []).append(r1)\n",
    "                    print(f\"   [LangExtract]: {le_summary[:100]}...\")\n",
    "\n",
    "            for m, s in sums.items():\n",
    "                if m != 'LangExtract':  # Already processed\n",
    "                    r1 = self.metrics.rouge_scores(d['reference_summary'], s)\n",
    "                    scores['summ'].setdefault(m, []).append(r1)\n",
    "                    if m == 'AraBART': print(f\"   [{m}]: {s[:100]}...\")\n",
    "            \n",
    "            # 2. NER\n",
    "            print(\"ğŸ·ï¸ NER:\")\n",
    "            ents = self.ner.extract_all(text)\n",
    "            \n",
    "            # Add LangExtract NER\n",
    "            if self.lang_extract.available:\n",
    "                le_entities = self.lang_extract.extract_entities(text)\n",
    "                if le_entities:\n",
    "                    ents['LangExtract'] = le_entities\n",
    "                    f1 = self.metrics.ner_metrics(d['entities'], le_entities)\n",
    "                    scores['ner'].setdefault('LangExtract', []).append(f1)\n",
    "\n",
    "            for m, e in ents.items():\n",
    "                if m != 'LangExtract':  # Already processed\n",
    "                    f1 = self.metrics.ner_metrics(d['entities'], e)\n",
    "                    scores['ner'].setdefault(m, []).append(f1)\n",
    "\n",
    "            c_ents = [f\"{x['text']}\" for x in ents.get('CAMeL', [])[:6]]\n",
    "            print(f\"   Entities found: {', '.join(c_ents)}...\")\n",
    "\n",
    "            # 3. Sentiment\n",
    "            print(\"ğŸ˜Š Sentiment:\")\n",
    "            pred_sent = self.sentiment.predict([text])[0]\n",
    "            # Normalize prediction for comparison\n",
    "            p_label = 'positive' if 'positive' in pred_sent or 'pos' in pred_sent else ('negative' if 'negative' in pred_sent or 'neg' in pred_sent else 'neutral')\n",
    "            t_label = d.get('sentiment', 'neutral')\n",
    "            print(f\"   True: {t_label} | Pred: {p_label}\")\n",
    "            scores['sent'].append(1 if p_label == t_label else 0)\n",
    "\n",
    "        # 4. Global Results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ† FINAL BENCHMARK SCORES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nğŸ“ SUMMARIZATION (ROUGE-1)\")\n",
    "        avgs = {k: np.mean(v) for k, v in scores['summ'].items()}\n",
    "        for k, v in sorted(avgs.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {k:<15} : {v:.4f}\")\n",
    "\n",
    "        print(\"\\nğŸ·ï¸ NER (F1 Score)\")\n",
    "        avgs_ner = {k: np.mean(v) for k, v in scores['ner'].items()}\n",
    "        for k, v in sorted(avgs_ner.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {k:<15} : {v:.4f}\")\n",
    "\n",
    "        print(f\"\\nğŸ˜Š SENTIMENT ACCURACY: {np.mean(scores['sent']):.2f}\")\n",
    "        \n",
    "        topics, coh = self.topics.run([d['text'] for d in data])\n",
    "        print(f\"\\nğŸ“Š TOPIC COHERENCE: {coh:.4f}\")\n",
    "        \n",
    "        # LangExtract Topic Modeling Evaluation\n",
    "        if self.lang_extract.available:\n",
    "            print(\"\\nğŸŒ LANGEXTRACT TOPIC ANALYSIS:\")\n",
    "            for i, d in enumerate(data):\n",
    "                le_topics = self.lang_extract.extract_topics(d['text'])\n",
    "                if le_topics:\n",
    "                    print(f\"   Doc {i+1} Topics: {', '.join(le_topics[:3])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef0f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# DATA (LARGE & FORMATTED)\n",
    "# =============================================\n",
    "def get_large_data():\n",
    "    return [\n",
    "        {\n",
    "            'text': \"\"\"Ø£Ø¹Ù„Ù†Øª Ø´Ø±ÙƒØ© Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŒ Ø¹Ù…Ù„Ø§Ù‚ Ø§Ù„Ù†ÙØ· Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ ÙˆØ£ÙƒØ¨Ø± Ø´Ø±ÙƒØ© Ø·Ø§Ù‚Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø³ÙˆÙ‚ÙŠØ©ØŒ Ø§Ù„ÙŠÙˆÙ… Ø¹Ù† ØªØ­Ù‚ÙŠÙ‚ Ù†ØªØ§Ø¦Ø¬ Ù…Ø§Ù„ÙŠØ© Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ© ÙˆØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚Ø© Ø®Ù„Ø§Ù„ Ø§Ù„Ø±Ø¨Ø¹ Ø§Ù„Ø«Ø§Ù„Ø« Ù…Ù† Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØŒ Ø­ÙŠØ« Ø¨Ù„ØºØª Ø§Ù„Ø£Ø±Ø¨Ø§Ø­ Ø§Ù„ØµØ§ÙÙŠØ© Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø§Ø¦Ø© ÙˆØ®Ù…Ø³ÙŠÙ† Ù…Ù„ÙŠØ§Ø± Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠØŒ Ø¨Ø²ÙŠØ§Ø¯Ø© Ù‚Ø¯Ø±Ù‡Ø§ Ø®Ù…Ø³Ø© ÙˆØ¹Ø´Ø±ÙˆÙ† Ø¨Ø§Ù„Ù…Ø§Ø¦Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„ÙØªØ±Ø© Ù†ÙØ³Ù‡Ø§ Ù…Ù† Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ù…Ø§Ø¶ÙŠ. Ø¬Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø§Ù„Ù…Ù‡Ù… Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ø¤ØªÙ…Ø± Ø§Ù„ØµØ­ÙÙŠ Ø§Ù„Ø°ÙŠ Ø¹Ù‚Ø¯Ù‡ Ø§Ù„Ø±Ø¦ÙŠØ³ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ Ù„Ù„Ø´Ø±ÙƒØ© Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³ Ø£Ù…ÙŠÙ† Ø­Ø³Ù† Ø§Ù„Ù†Ø§ØµØ± ÙÙŠ Ø§Ù„Ù…Ù‚Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø´Ø±ÙƒØ© Ø¨Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†. ÙˆØ£ÙˆØ¶Ø­ Ø§Ù„Ù†Ø§ØµØ± Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ¹ÙƒØ³ Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„ÙŠ ÙˆÙ‚Ø¯Ø±Ø© Ø§Ù„Ø´Ø±ÙƒØ© Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©. ÙˆØ£Ø¶Ø§Ù Ø£Ù† Ø§Ù„Ø´Ø±ÙƒØ© ÙˆÙ‚Ø¹Øª Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø´Ø±Ø§ÙƒØ© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¶Ø®Ù…Ø© Ù…Ø¹ Ø´Ø±ÙƒØ© ØªÙˆØªØ§Ù„ Ø¥Ù†Ø±Ø¬ÙŠØ² Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© ÙˆØ´Ø±ÙƒØ© Ø´Ù„ Ø§Ù„Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ© Ù„ØªØ·ÙˆÙŠØ± Ø­Ù‚ÙˆÙ„ Ø§Ù„ØºØ§Ø² Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ. ÙˆÙÙŠ Ø³ÙŠØ§Ù‚ Ù…ØªØµÙ„ Ø¨Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„ÙˆØ·Ù†ÙŠØŒ Ø³Ø¬Ù„ Ù…Ø¤Ø´Ø± Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ (ØªØ¯Ø§ÙˆÙ„) Ø§Ù†Ø®ÙØ§Ø¶Ø§Ù‹ Ø­Ø§Ø¯Ø§Ù‹ Ø¨Ù†Ø³Ø¨Ø© 2%ØŒ Ù…ØªØ£Ø«Ø±Ø§Ù‹ Ø¨ØªØ±Ø§Ø¬Ø¹ Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø¨Ù†ÙˆÙƒ. ÙˆÙÙŠ Ø®Ø·ÙˆØ© Ù…ÙØ§Ø¬Ø¦Ø©ØŒ Ø£Ø¹Ù„Ù† Ù…ØµØ±Ù Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠØŒ Ø£Ø­Ø¯ Ø£ÙƒØ¨Ø± Ø§Ù„Ø¨Ù†ÙˆÙƒ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©ØŒ Ø¹Ù† Ø§Ù†Ø®ÙØ§Ø¶ Ø·ÙÙŠÙ ÙÙŠ Ø£Ø±Ø¨Ø§Ø­Ù‡ Ø§Ù„ÙØµÙ„ÙŠØ© Ø¨Ø³Ø¨Ø¨ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø®ØµØµØ§Øª.\"\"\",\n",
    "            'reference_summary': \"Ø£Ø±Ø¨Ø§Ø­ Ù‚ÙŠØ§Ø³ÙŠØ© Ù„Ø£Ø±Ø§Ù…ÙƒÙˆ ÙˆØ´Ø±Ø§ÙƒØ§Øª Ù…Ø¹ ØªÙˆØªØ§Ù„ ÙˆØ´Ù„ØŒ ÙˆØ³Ø· ØªØ±Ø§Ø¬Ø¹ Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ ÙˆØ£Ø±Ø¨Ø§Ø­ Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ.\",\n",
    "            'entities': [{'text': 'Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©', 'label': 'ORG'}, {'text': 'Ø£Ù…ÙŠÙ† Ø§Ù„Ù†Ø§ØµØ±', 'label': 'PERS'}, {'text': 'Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†', 'label': 'LOC'}, {'text': 'ØªÙˆØªØ§Ù„ Ø¥Ù†Ø±Ø¬ÙŠØ²', 'label': 'ORG'}, {'text': 'Ø´Ù„', 'label': 'ORG'}, {'text': 'Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ', 'label': 'ORG'}, {'text': 'Ø¨Ù†Ùƒ Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ', 'label': 'ORG'}],\n",
    "            'sentiment': 'mixed'\n",
    "        },\n",
    "        {\n",
    "            'text': \"\"\"Ø§Ø®ØªØªÙ…Øª Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø·Ø§Ø±Ø¦Ø© Ø£Ø¹Ù…Ø§Ù„Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ© Ø¹Ù…Ø§Ù†ØŒ ÙˆØ³Ø· Ø­Ø¶ÙˆØ± Ø±ÙÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ù…Ù† Ù‚Ø§Ø¯Ø© Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. ÙˆÙ‚Ø¯ Ù‡ÙŠÙ…Ù† Ø¹Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…ØªÙØ¬Ø± ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©. ÙˆØ£ÙƒØ¯ Ø§Ù„Ø¹Ø§Ù‡Ù„ Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠ Ø§Ù„Ù…Ù„Ùƒ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø«Ø§Ù†ÙŠ ÙÙŠ ÙƒÙ„Ù…ØªÙ‡ Ø§Ù„Ø§ÙØªØªØ§Ø­ÙŠØ© Ø¹Ù„Ù‰ Ø¶Ø±ÙˆØ±Ø© Ø§Ù„ØªØ¶Ø§Ù…Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø±Ø§Ù‡Ù†Ø©. ÙˆØ¹Ù„Ù‰ Ù‡Ø§Ù…Ø´ Ø§Ù„Ù‚Ù…Ø©ØŒ Ø¹Ù‚Ø¯ Ø§Ù„Ù…Ù„ÙƒØ¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ø¬ØªÙ…Ø§Ø¹Ø§Øª Ø«Ù†Ø§Ø¦ÙŠØ© Ù…ØºÙ„Ù‚Ø© Ù…Ø¹ ÙˆÙ„ÙŠ Ø§Ù„Ø¹Ù‡Ø¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ Ø§Ù„Ø£Ù…ÙŠØ± Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù†ØŒ ÙˆØ§Ù„Ø±Ø¦ÙŠØ³ Ø§Ù„Ù…ØµØ±ÙŠ Ø¹Ø¨Ø¯Ø§Ù„ÙØªØ§Ø­ Ø§Ù„Ø³ÙŠØ³ÙŠØŒ Ø­ÙŠØ« ØªÙ… Ø¨Ø­Ø« Ø³Ø¨Ù„ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…ÙˆØ§Ù‚Ù. ÙˆÙ†Ø§Ù‚Ø´Øª Ø§Ù„Ù‚Ù…Ø© Ø¨Ø§Ø³ØªÙØ§Ø¶Ø© Ø§Ù„Ø£ÙˆØ¶Ø§Ø¹ Ø§Ù„Ù…Ø£Ø³Ø§ÙˆÙŠØ© ÙÙŠ Ø³ÙˆØ±ÙŠØ§ ÙˆØ§Ù„ÙŠÙ…Ù†ØŒ Ø¯Ø§Ø¹ÙŠØ© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø¯ÙˆÙ„ÙŠ Ø¥Ù„Ù‰ ØªØ­Ù…Ù„ Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§ØªÙ‡ Ù„Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„ØµØ±Ø§Ø¹Ø§Øª ÙˆÙˆÙ‚Ù Ù†Ø²ÙŠÙ Ø§Ù„Ø¯Ù….\"\"\",\n",
    "            'reference_summary': \"Ø®ØªØ§Ù… Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ø¹Ù…Ø§Ù† Ø¨Ø¯Ø¹ÙˆØ§Øª Ù„Ù„ØªØ¶Ø§Ù…Ù†ØŒ ÙˆÙ„Ù‚Ø§Ø¡Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…Ù„ÙƒØ¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ ÙˆÙ…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù† ÙˆØ§Ù„Ø³ÙŠØ³ÙŠ.\",\n",
    "            'entities': [{'text': 'Ø¹Ù…Ø§Ù†', 'label': 'LOC'}, {'text': 'Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø«Ø§Ù†ÙŠ', 'label': 'PERS'}, {'text': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù†', 'label': 'PERS'}, {'text': 'Ø¹Ø¨Ø¯Ø§Ù„ÙØªØ§Ø­ Ø§Ù„Ø³ÙŠØ³ÙŠ', 'label': 'PERS'}, {'text': 'Ø³ÙˆØ±ÙŠØ§', 'label': 'LOC'}, {'text': 'Ø§Ù„ÙŠÙ…Ù†', 'label': 'LOC'}],\n",
    "            'sentiment': 'neutral'\n",
    "        },\n",
    "        {\n",
    "            'text': \"\"\"ØªØ´Ù‡Ø¯ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø·ÙØ±Ø© ØªÙ‚Ù†ÙŠØ© Ù‡Ø§Ø¦Ù„Ø©ØŒ Ø­ÙŠØ« Ø£Ø¹Ù„Ù†Øª Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„ÙƒØ¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ù„Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ© (ÙƒØ§ÙˆØ³Øª) Ø¹Ù† Ø¥Ø·Ù„Ø§Ù‚ Ù…Ø¨Ø§Ø¯Ø±Ø© ÙˆØ·Ù†ÙŠØ© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ù„ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø´Ø±ÙƒØ§Øª Ø¹Ø§Ù„Ù…ÙŠØ© Ù…Ø«Ù„ Ø¬ÙˆØ¬Ù„ ÙˆÙ…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª. ÙˆØªÙ‡Ø¯Ù Ø§Ù„Ù…Ø¨Ø§Ø¯Ø±Ø© Ø¥Ù„Ù‰ ØªØ¹Ø±ÙŠØ¨ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù„ØºÙˆÙŠØ© ØªØ®Ø¯Ù… Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. ÙˆÙÙŠ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµØ­ÙŠØŒ Ø­Ù‚Ù‚ Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ù…Ù„Ùƒ ÙÙŠØµÙ„ Ø§Ù„ØªØ®ØµØµÙŠ ÙˆÙ…Ø±ÙƒØ² Ø§Ù„Ø£Ø¨Ø­Ø§Ø« Ø¥Ù†Ø¬Ø§Ø²Ø§Ù‹ Ø·Ø¨ÙŠØ§Ù‹ ØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚ØŒ Ø­ÙŠØ« Ù†Ø¬Ø­ ÙØ±ÙŠÙ‚ Ø·Ø¨ÙŠ Ø¨Ù‚ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯ÙƒØªÙˆØ± Ø³Ø¹ÙˆØ¯ Ø§Ù„Ø´Ù…Ø±ÙŠ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ø§Ø¬ Ø¬ÙŠÙ†ÙŠ Ù…ØªØ·ÙˆØ± Ù„Ù…Ø±Ø¶Ù‰ Ø§Ù„Ø³Ø±Ø·Ø§Ù†ØŒ Ù…Ù…Ø§ Ø£Ø¯Ù‰ Ø¥Ù„Ù‰ Ù†Ø³Ø¨ Ø´ÙØ§Ø¡ Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹. ÙˆÙ‚Ø¯ Ø£Ø´Ø§Ø¯ ÙˆØ²ÙŠØ± Ø§Ù„ØµØ­Ø© ÙÙ‡Ø¯ Ø§Ù„Ø¬Ù„Ø§Ø¬Ù„ Ø¨Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø§Ù„ÙƒØ¨ÙŠØ±.\"\"\",\n",
    "            'reference_summary': \"Ø£Ø·Ù„Ù‚Øª ÙƒØ§ÙˆØ³Øª Ù…Ø¨Ø§Ø¯Ø±Ø© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ Ø¬ÙˆØ¬Ù„ ÙˆÙ…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª. Ø­Ù‚Ù‚ Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ù…Ù„Ùƒ ÙÙŠØµÙ„ Ø§Ù„ØªØ®ØµØµÙŠ Ø¥Ù†Ø¬Ø§Ø²Ø§Ù‹ Ø·Ø¨ÙŠØ§Ù‹ Ø¨Ù‚ÙŠØ§Ø¯Ø© Ø³Ø¹ÙˆØ¯ Ø§Ù„Ø´Ù…Ø±ÙŠ.\",\n",
    "            'entities': [{'text': 'ÙƒØ§ÙˆØ³Øª', 'label': 'ORG'}, {'text': 'Ø¬ÙˆØ¬Ù„', 'label': 'ORG'}, {'text': 'Ù…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª', 'label': 'ORG'}, {'text': 'Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ù…Ù„Ùƒ ÙÙŠØµÙ„ Ø§Ù„ØªØ®ØµØµÙŠ', 'label': 'ORG'}, {'text': 'Ø³Ø¹ÙˆØ¯ Ø§Ù„Ø´Ù…Ø±ÙŠ', 'label': 'PERS'}, {'text': 'ÙÙ‡Ø¯ Ø§Ù„Ø¬Ù„Ø§Ø¬Ù„', 'label': 'PERS'}],\n",
    "            'sentiment': 'positive'\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a56ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ ARABIC NLP PIPELINE: BENCHMARK EDITION\n",
      "======================================================================\n",
      "  ğŸ“š Loading CAMeL Morphology...\n",
      "  ğŸ·ï¸ Loading NER Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/user/.camel_tools/data/ner/arabert were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“ Loading Summarization Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š Topic Modeling: Gensim\n",
      "  ğŸŒ Loading Ollama with gemma3:4b (LLM-based Multilingual Model)...\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ DETAILED ANALYSIS (LARGE DOCS)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ Document 1 (148 words)\n",
      "ğŸ“ Summarization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [LangExtract]: Here's a concise summary of the text in Arabic:\n",
      "\n",
      "**Ø¨Ù„ØºØª Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø£Ø±Ø¨Ø§Ø­Ù‹Ø§ ØµØ§ÙÙŠØ© Ù‚ÙŠØ§Ø³ÙŠØ© ØªØ¬Ø§ÙˆØ²Øª ...\n",
      "   [AraBART]: Ø§Ø¹Ù„Ù†Øª Ø´Ø±ÙƒÙ‡ Ø§Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠÙ‡ØŒ Ø¹Ù…Ù„Ø§Ù‚ Ø§Ù„Ù†ÙØ· Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ ÙˆØ§ÙƒØ¨Ø± Ø´Ø±ÙƒÙ‡ Ø·Ø§Ù‚Ù‡ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ù‚ÙŠÙ…Ù‡ Ø§Ù„Ø³ÙˆÙ‚ÙŠÙ‡ØŒ Ø§Ù„ÙŠ...\n",
      "ğŸ·ï¸ NER:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Entities found: Ø§Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©, Ø±ÙŠØ§Ù„, Ø§Ù…ÙŠÙ† Ø­Ø³Ù† Ø§Ù„Ù†Ø§ØµØ±, Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†, Ø§Ù„Ù†Ø§ØµØ±, ØªÙˆØªØ§Ù„ Ø§Ù†Ø±Ø¬ÙŠØ²...\n",
      "ğŸ˜Š Sentiment:\n",
      "   True: mixed | Pred: positive\n",
      "\n",
      "ğŸ“‚ Document 2 (87 words)\n",
      "ğŸ“ Summarization:\n",
      "   [LangExtract]: Hereâ€™s a concise summary of the Arabic text in Arabic:\n",
      "\n",
      "**ØªÙ„Ù‚Øª Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø·Ø§Ø±Ø¦Ø© ÙÙŠ Ø§Ù„Ø£Ø±Ø¯Ù† Ø§Ù„Ø¶ÙˆØ¡...\n",
      "   [AraBART]: Ø§Ø®ØªØªÙ…Øª Ø§Ù„Ù‚Ù…Ù‡ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡ Ø§Ù„Ø·Ø§Ø±Ø¦Ù‡ Ø§Ø¹Ù…Ø§Ù„Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§ØµÙ…Ù‡ Ø§Ù„Ø§Ø±Ø¯Ù†ÙŠÙ‡ Ø¹Ù…Ø§Ù†ØŒ ÙˆØ³Ø· Ø­Ø¶ÙˆØ± Ø±ÙÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠ Ù…Ù† Ù‚Ø§Ø¯Ù‡ Ø§Ù„Ø¯ÙˆÙ„ Ø§...\n",
      "ğŸ·ï¸ NER:\n",
      "   Entities found: Ø¹Ù…Ø§Ù†, Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø§Ù„Ø«Ø§Ù†ÙŠ, Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ù…Ø§Ù†, Ø¹Ø¨Ø¯Ø§Ù„ÙØªØ§Ø­ Ø§Ù„Ø³ÙŠØ³ÙŠ, Ø³ÙˆØ±ÙŠØ§, ÙˆØ§Ù„ÙŠÙ…Ù†...\n",
      "ğŸ˜Š Sentiment:\n",
      "   True: neutral | Pred: neutral\n",
      "\n",
      "ğŸ“‚ Document 3 (86 words)\n",
      "ğŸ“ Summarization:\n",
      "   [LangExtract]: Here's a concise summary of the text in Arabic:\n",
      "\n",
      "**ØªÙØ´Ù‡Ø¯ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø·ÙØ±Ø© ØªÙ‚Ù†ÙŠØ© Ø¨ÙØ¶Ù„ Ù…Ø¨Ø§Ø¯Ø±Ø© ÙƒØ§ÙˆØ³Øª Ù„Ù„Ø°ÙƒØ§...\n",
      "   [AraBART]: ØªØ´Ù‡Ø¯ Ø§Ù„Ù…Ù…Ù„ÙƒÙ‡ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠÙ‡ Ø·ÙØ±Ù‡ ØªÙ‚Ù†ÙŠÙ‡ Ù‡Ø§Ø¦Ù„Ù‡ØŒ Ø­ÙŠØ« Ø§Ø¹Ù„Ù†Øª Ø¬Ø§Ù…Ø¹Ù‡ Ø§Ù„Ù…Ù„ÙƒØ¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ù„Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙ‚Ù†ÙŠÙ‡ (ÙƒØ§ÙˆØ³Øª)...\n",
      "ğŸ·ï¸ NER:\n",
      "   Entities found: Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©, Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„ÙƒØ¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ù„Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ©, ÙƒØ§ÙˆØ³Øª, Ø¬ÙˆØ¬Ù„, ÙˆÙ…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª, Ø§Ù„Ù…Ù„Ùƒ ÙÙŠØµÙ„ Ø§Ù„ØªØ®ØµØµÙŠ...\n",
      "ğŸ˜Š Sentiment:\n",
      "   True: positive | Pred: positive\n",
      "\n",
      "======================================================================\n",
      "ğŸ† FINAL BENCHMARK SCORES\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ SUMMARIZATION (ROUGE-1)\n",
      "  mT5-XLSum       : 0.2083\n",
      "  Sumy-LexRank    : 0.1828\n",
      "  Sumy-LSA        : 0.1828\n",
      "  AraBART         : 0.1798\n",
      "  LangExtract     : 0.1526\n",
      "\n",
      "ğŸ·ï¸ NER (F1 Score)\n",
      "  CAMeL           : 0.8413\n",
      "  LangExtract     : 0.7564\n",
      "  Stanza          : 0.7143\n",
      "  Hatmimoha       : 0.6088\n",
      "\n",
      "ğŸ˜Š SENTIMENT ACCURACY: 0.67\n",
      "\n",
      "ğŸ“Š TOPIC COHERENCE: 0.5343\n",
      "\n",
      "ğŸŒ LANGEXTRACT TOPIC ANALYSIS:\n",
      "   Doc 1 Topics: Hereâ€™s a list of 3-5 key topics/phrases representing the main subjects of the Arabic text:, **Aramco Financial Results:** (Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ù„Ø£Ø±Ø§Ù…ÙƒÙˆ) - The core of the text focuses on Aramcoâ€™s exceptional profits and performance., **Strategic Partnerships:** (Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø´Ø±Ø§ÙƒØ© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©) -  The text highlights Aramcoâ€™s new partnerships with TotalEnergies and Shell.\n",
      "   Doc 2 Topics: Hereâ€™s a list of 3-5 key topics/phrases represented in the Arabic text:, **The Arab Emergency Summit:** This is the overarching event and the primary focus of the text., **The Situation in the Middle East (Explosive Situation):** The text repeatedly highlights the instability and conflict within the region as the central concern.\n",
      "   Doc 3 Topics: Hereâ€™s a list of 3-5 key topics/phrases representing the main subjects of the Arabic text:, **Artificial Intelligence (AI) Initiative:** The central focus is the national AI initiative launched by KAUST., **Arabic Localization of AI:** A key goal of the initiative is to adapt AI technologies for the Arab region.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    UltimatePipeline().run(get_large_data())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
