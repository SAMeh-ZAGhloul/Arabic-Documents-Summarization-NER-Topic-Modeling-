{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Install CAMeL Tools\n",
    "pip install camel-tools scikit-learn networkx numpy\n",
    "\n",
    "# Download required models\n",
    "camel_data -i ner-arabert                    # NER (541 MB)\n",
    "camel_data -i sentiment-analysis-arabert     # Sentiment (541 MB)\n",
    "camel_data -i morphology-db-msa-r13          # Morphology (40 MB)\n",
    "camel_data -i disambig-mle-calima-msa-r13    # Disambiguation (88 MB)\n",
    "\n",
    "Components:\n",
    "- Preprocessing & Lemmatization (CAMeL Tools)\n",
    "- Named Entity Recognition (AraBERT)\n",
    "- Sentiment Analysis (CAMeL Tools)\n",
    "- Topic Modeling (LDA)\n",
    "- Extractive Summarization (TextRank)\n",
    "- Abstractive Summarization (mT5 & AraT5 & AraBART)\n",
    "- Accuracy benchmarks for all components\n",
    "- ROUGE scores for summarization\n",
    "- F1/Precision/Recall for NER\n",
    "- Accuracy for Sentiment\n",
    "- Coherence for Topic Modeling\n",
    "\n",
    "Summarization:\n",
    "  1. Sumy-LexRank, TextRank, LSA\n",
    "  2. TF-IDF Baseline\n",
    "  3. mT5-XLSum (Abstractive)\n",
    "  4. AraBART (Abstractive)\n",
    "\n",
    "NER Comparison:\n",
    "  1. CAMeL Tools (AraBERT)\n",
    "  2. Stanford Stanza\n",
    "  3. Hatmimoha (BERT)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "# =============================================\n",
    "# ๐๏ธ SETUP & IMPORTS\n",
    "# =============================================\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"โฌ๏ธ Downloading NLTK data...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline as hf_pipeline\n",
    ")\n",
    "\n",
    "# CAMeL Tools\n",
    "from camel_tools.utils.normalize import normalize_unicode, normalize_alef_ar, normalize_alef_maksura_ar, normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "\n",
    "# Optional Libraries\n",
    "try:\n",
    "    import sumy\n",
    "    from sumy.parsers.plaintext import PlaintextParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer\n",
    "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "    from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "    from sumy.summarizers.lsa import LsaSummarizer\n",
    "    SUMY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SUMY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "    from gensim.models import LdaModel\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "    GENSIM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GENSIM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import stanza\n",
    "    STANZA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    STANZA_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE_ID = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# =============================================\n",
    "# 1. METRICS\n",
    "# =============================================\n",
    "class EvaluationMetrics:\n",
    "    @staticmethod\n",
    "    def normalize_arabic(text):\n",
    "        text = normalize_unicode(text)\n",
    "        text = dediac_ar(text)\n",
    "        text = normalize_alef_ar(text)\n",
    "        text = normalize_alef_maksura_ar(text)\n",
    "        text = normalize_teh_marbuta_ar(text)\n",
    "        return re.sub(r'\\bุงู', '', text).lower().strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def rouge_scores(reference, hypothesis):\n",
    "        def tokenize(text):\n",
    "            text = EvaluationMetrics.normalize_arabic(text)\n",
    "            return [t for t in simple_word_tokenize(text) if len(t) > 1 and not t.isdigit()]\n",
    "        \n",
    "        ref_tokens = tokenize(reference)\n",
    "        hyp_tokens = tokenize(hypothesis)\n",
    "        if not ref_tokens or not hyp_tokens: return 0.0\n",
    "        \n",
    "        overlap = sum((Counter(ref_tokens) & Counter(hyp_tokens)).values())\n",
    "        p = overlap / len(hyp_tokens)\n",
    "        r = overlap / len(ref_tokens)\n",
    "        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def ner_metrics(true_entities, pred_entities):\n",
    "        def norm(e): return EvaluationMetrics.normalize_arabic(e['text'])\n",
    "        true_set = set((norm(e), e['label']) for e in true_entities)\n",
    "        partial_tp = 0\n",
    "        for p in pred_entities:\n",
    "            p_norm = norm(p)\n",
    "            for t in true_entities:\n",
    "                t_norm = norm(t)\n",
    "                if p['label'] == t['label'] and (p_norm in t_norm or t_norm in p_norm):\n",
    "                    partial_tp += 1\n",
    "                    break\n",
    "        prec = partial_tp / len(pred_entities) if pred_entities else 0\n",
    "        rec = partial_tp / len(true_entities) if true_entities else 0\n",
    "        return 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "\n",
    "class ArabicPreprocessor:\n",
    "    STOPWORDS = set(\"ูู ูู ุฅูู ุนูู ุฃู ูุฐุง ูุฐู ุงูุฐู ุงูุชู ููู ูุงู ุจูุง ูู ุจุฃู ููุงู ุนู ุญูุซ ู ูุง ุงู ุจ ู ู ู ู ููุง ูุฐุง ูุฃู ุญุชู ููุน ุฏูู ุฃู ููุง ูู ุจุนุฏ ูุจู ุนูุฏ ุจูู ููุง ุฃูุถุง ุซู ูู ูู ุฅุฐุง ููู ูุง ูู ุฃู ูู ูู ูุญู ุฃูุง ุฃูุช ูููู ุชููู ูุงูุช ุนููู ุฅููู ููู ููู ุจู ุฐูู ุชูู ูุคูุงุก ุฃููุฆู ุนุงู ุจุนุถ ุฌููุน ุฃูุซุฑ ูุนุธู ุบูุฑ ุฎูุงู ุถูู ูุญู ุญูู ูุฏ ูุงู ูููู ูุงููุง ููุงู ูุงูุช ููู ููุฏ ููุง ููู ููู ููู ููู ูููู ูุฅู ุฅูุง ุฃูุง\".split())\n",
    "    def __init__(self):\n",
    "        print(\"  ๐ Loading CAMeL Morphology...\")\n",
    "        self.db = MorphologyDB.builtin_db('calima-msa-r13')\n",
    "        self.analyzer = Analyzer(self.db)\n",
    "    def normalize(self, text):\n",
    "        return normalize_teh_marbuta_ar(normalize_alef_maksura_ar(normalize_alef_ar(dediac_ar(normalize_unicode(text)))))\n",
    "    def preprocess(self, text):\n",
    "        text = self.normalize(text)\n",
    "        tokens = simple_word_tokenize(text)\n",
    "        lemmas = []\n",
    "        for t in tokens:\n",
    "            if len(t) < 2 or t in self.STOPWORDS: continue\n",
    "            analyses = self.analyzer.analyze(t)\n",
    "            lemma = analyses[0].get('lex', t) if analyses else t\n",
    "            lemmas.append(re.sub(r'_\\d+$', '', lemma))\n",
    "        return [dediac_ar(l) for l in lemmas]\n",
    "\n",
    "# =============================================\n",
    "# 2. MODELS (NER, Summ, Topics)\n",
    "# =============================================\n",
    "class ArabicNER:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        print(\"  ๐ท๏ธ Loading NER Models...\")\n",
    "        # CAMeL\n",
    "        try:\n",
    "            path = \"/Users/user/.camel_tools/data/ner/arabert\"\n",
    "            self.c_tok = AutoTokenizer.from_pretrained(path)\n",
    "            self.c_mod = AutoModelForTokenClassification.from_pretrained(path)\n",
    "            self.c_lbl = {0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PERS', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PERS', 8: 'O'}\n",
    "            self.models['CAMeL'] = True\n",
    "        except: pass\n",
    "        # Hatmimoha\n",
    "        try:\n",
    "            self.h_pipe = hf_pipeline(\"ner\", model=\"hatmimoha/arabic-ner\", aggregation_strategy=\"simple\", device=DEVICE_ID)\n",
    "            self.models['Hatmimoha'] = True\n",
    "        except: pass\n",
    "        # Stanza\n",
    "        try:\n",
    "            if STANZA_AVAILABLE:\n",
    "                stanza.download('ar', processors='tokenize,ner', verbose=False)\n",
    "                self.s_nlp = stanza.Pipeline('ar', processors='tokenize,ner', verbose=False)\n",
    "                self.models['Stanza'] = True\n",
    "        except: pass\n",
    "\n",
    "    def extract_all(self, text):\n",
    "        res = {}\n",
    "        if 'CAMeL' in self.models: res['CAMeL'] = self._camel(text)\n",
    "        if 'Hatmimoha' in self.models: res['Hatmimoha'] = self._hat(text)\n",
    "        if 'Stanza' in self.models: res['Stanza'] = self._stanza(text)\n",
    "        return res\n",
    "\n",
    "    def _camel(self, text):\n",
    "        ents = []\n",
    "        inp = self.c_tok(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad(): preds = torch.argmax(self.c_mod(**inp).logits, dim=2)[0]\n",
    "        toks = self.c_tok.convert_ids_to_tokens(inp['input_ids'][0])\n",
    "        lbls = [self.c_lbl[p.item()] for p in preds]\n",
    "        curr_t, curr_l = [], None\n",
    "        for t, l in zip(toks, lbls):\n",
    "            if t in ['[CLS]', '[SEP]']: continue\n",
    "            if t.startswith('##'): \n",
    "                if curr_t: curr_t[-1] += t[2:]\n",
    "            elif l.startswith('B-'):\n",
    "                if curr_t: ents.append({'text': ' '.join(curr_t), 'label': curr_l})\n",
    "                curr_t, curr_l = [t], l[2:]\n",
    "            elif l.startswith('I-') and curr_l: curr_t.append(t)\n",
    "            else:\n",
    "                if curr_t: ents.append({'text': ' '.join(curr_t), 'label': curr_l})\n",
    "                curr_t, curr_l = [], None\n",
    "        return ents\n",
    "\n",
    "    def _hat(self, text):\n",
    "        try:\n",
    "            r = self.h_pipe(text)\n",
    "            return [{'text': x['word'], 'label': {'PERSON':'PERS','LOCATION':'LOC','ORGANIZATION':'ORG'}.get(x['entity_group'],'MISC')} for x in r]\n",
    "        except: return []\n",
    "\n",
    "    def _stanza(self, text):\n",
    "        try:\n",
    "            doc = self.s_nlp(text)\n",
    "            return [{'text': e.text, 'label': {'PER':'PERS'}.get(e.type, e.type)} for e in doc.ents]\n",
    "        except: return []\n",
    "\n",
    "class ArabicSummarizer:\n",
    "    def __init__(self, prep):\n",
    "        self.prep = prep\n",
    "        self.sumy = {}\n",
    "        if SUMY_AVAILABLE:\n",
    "            print(\"  ๐ Loading Summarization Models...\")\n",
    "            self.sumy = {'Sumy-LexRank': LexRankSummarizer, 'Sumy-LSA': LsaSummarizer}\n",
    "        self.neural = {}\n",
    "        self._load('AraBART', 'moussaKam/AraBART', 'seq2seq')\n",
    "        self._load('mT5-XLSum', 'csebuetnlp/mT5_multilingual_XLSum', 'pipeline')\n",
    "\n",
    "    def _load(self, name, path, type):\n",
    "        try:\n",
    "            if type == 'pipeline': self.neural[name] = {'mod': hf_pipeline(\"summarization\", model=path, device=DEVICE_ID), 'type': 'pipe'}\n",
    "            else:\n",
    "                tok = AutoTokenizer.from_pretrained(path)\n",
    "                mod = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "                mod.to(DEVICE)\n",
    "                self.neural[name] = {'tok': tok, 'mod': mod, 'type': 'seq'}\n",
    "        except: pass\n",
    "\n",
    "    def summarize(self, text):\n",
    "        res = {}\n",
    "        # Sumy\n",
    "        parser = PlaintextParser.from_string(text, SumyTokenizer(\"english\"))\n",
    "        for n, cls in self.sumy.items():\n",
    "            try: res[n] = ' '.join([str(s) for s in cls()(parser.document, 7)])\n",
    "            except: pass\n",
    "        # Neural\n",
    "        clean = ' '.join(self.prep.normalize(text).split())[:4000]\n",
    "        for n, c in self.neural.items():\n",
    "            try:\n",
    "                if c['type'] == 'pipe': res[n] = c['mod'](clean, max_length=150, min_length=50, truncation=True)[0]['summary_text']\n",
    "                else:\n",
    "                    inp = c['tok'](clean, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "                    out = c['mod'].generate(**inp, max_length=150, min_length=50, num_beams=4)\n",
    "                    res[n] = c['tok'].decode(out[0], skip_special_tokens=True)\n",
    "            except: pass\n",
    "        return res\n",
    "\n",
    "class TopicModeler:\n",
    "    def __init__(self, prep):\n",
    "        self.prep = prep\n",
    "        print(f\"  ๐ Topic Modeling: {'Gensim' if GENSIM_AVAILABLE else 'N/A'}\")\n",
    "\n",
    "    def run(self, docs):\n",
    "        if not GENSIM_AVAILABLE: return None, 0\n",
    "        texts = [self.prep.preprocess(d) for d in docs]\n",
    "        dic = corpora.Dictionary(texts)\n",
    "        dic.filter_extremes(no_below=1, no_above=0.9)\n",
    "        corpus = [dic.doc2bow(t) for t in texts]\n",
    "        lda = LdaModel(corpus, num_topics=3, id2word=dic, passes=20, random_state=42)\n",
    "        return lda.print_topics(num_words=5), CoherenceModel(model=lda, texts=texts, dictionary=dic, coherence='c_v').get_coherence()\n",
    "\n",
    "# =============================================\n",
    "# 3. PIPELINE EXECUTION\n",
    "# =============================================\n",
    "class UltimatePipeline:\n",
    "    def __init__(self):\n",
    "        print(\"=\"*70)\n",
    "        print(\"๐ ARABIC NLP PIPELINE: BENCHMARK EDITION\")\n",
    "        print(\"=\"*70)\n",
    "        self.prep = ArabicPreprocessor()\n",
    "        self.ner = ArabicNER()\n",
    "        self.summ = ArabicSummarizer(self.prep)\n",
    "        self.topics = TopicModeler(self.prep)\n",
    "        self.metrics = EvaluationMetrics()\n",
    "        self.sentiment = SentimentAnalyzer.pretrained()\n",
    "\n",
    "    def run(self, data):\n",
    "        scores = {'summ': {}, 'ner': {}, 'sent': []}\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"๐ DETAILED ANALYSIS (LARGE DOCS)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for i, d in enumerate(data):\n",
    "            text = d['text']\n",
    "            print(f\"\\n๐ Document {i+1} ({len(text.split())} words)\")\n",
    "            \n",
    "            # 1. Summarization\n",
    "            print(\"๐ Summarization:\")\n",
    "            sums = self.summ.summarize(text)\n",
    "            for m, s in sums.items():\n",
    "                r1 = self.metrics.rouge_scores(d['reference_summary'], s)\n",
    "                scores['summ'].setdefault(m, []).append(r1)\n",
    "                if m == 'AraBART': print(f\"   [{m}]: {s[:100]}...\")\n",
    "            \n",
    "            # 2. NER\n",
    "            print(\"๐ท๏ธ NER (CAMeL):\")\n",
    "            ents = self.ner.extract_all(text)\n",
    "            for m, e in ents.items():\n",
    "                f1 = self.metrics.ner_metrics(d['entities'], e)\n",
    "                scores['ner'].setdefault(m, []).append(f1)\n",
    "            \n",
    "            c_ents = [f\"{x['text']}\" for x in ents.get('CAMeL', [])[:6]]\n",
    "            print(f\"   Entities found: {', '.join(c_ents)}...\")\n",
    "\n",
    "            # 3. Sentiment\n",
    "            print(\"๐ Sentiment:\")\n",
    "            pred_sent = self.sentiment.predict([text])[0]\n",
    "            # Normalize prediction for comparison\n",
    "            p_label = 'positive' if 'positive' in pred_sent or 'pos' in pred_sent else ('negative' if 'negative' in pred_sent or 'neg' in pred_sent else 'neutral')\n",
    "            t_label = d.get('sentiment', 'neutral')\n",
    "            print(f\"   True: {t_label} | Pred: {p_label}\")\n",
    "            scores['sent'].append(1 if p_label == t_label else 0)\n",
    "\n",
    "        # 4. Global Results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"๐ FINAL BENCHMARK SCORES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\n๐ SUMMARIZATION (ROUGE-1)\")\n",
    "        avgs = {k: np.mean(v) for k, v in scores['summ'].items()}\n",
    "        for k, v in sorted(avgs.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {k:<15} : {v:.4f}\")\n",
    "\n",
    "        print(\"\\n๐ท๏ธ NER (F1 Score)\")\n",
    "        avgs_ner = {k: np.mean(v) for k, v in scores['ner'].items()}\n",
    "        for k, v in sorted(avgs_ner.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {k:<15} : {v:.4f}\")\n",
    "\n",
    "        print(f\"\\n๐ SENTIMENT ACCURACY: {np.mean(scores['sent']):.2f}\")\n",
    "        \n",
    "        topics, coh = self.topics.run([d['text'] for d in data])\n",
    "        print(f\"\\n๐ TOPIC COHERENCE: {coh:.4f}\")\n",
    "\n",
    "# =============================================\n",
    "# DATA (LARGE & FORMATTED)\n",
    "# =============================================\n",
    "def get_large_data():\n",
    "    return [\n",
    "        {\n",
    "            'text': \"\"\"ุฃุนููุช ุดุฑูุฉ ุฃุฑุงููู ุงูุณุนูุฏูุฉุ ุนููุงู ุงูููุท ุงูุนุงููู ูุฃูุจุฑ ุดุฑูุฉ ุทุงูุฉ ูู ุงูุนุงูู ูู ุญูุซ ุงููููุฉ ุงูุณูููุฉุ ุงูููู ุนู ุชุญููู ูุชุงุฆุฌ ูุงููุฉ ุงุณุชุซูุงุฆูุฉ ูุบูุฑ ูุณุจููุฉ ุฎูุงู ุงูุฑุจุน ุงูุซุงูุซ ูู ุงูุนุงู ุงูุญุงููุ ุญูุซ ุจูุบุช ุงูุฃุฑุจุงุญ ุงูุตุงููุฉ ุฃูุซุฑ ูู ูุงุฆุฉ ูุฎูุณูู ูููุงุฑ ุฑูุงู ุณุนูุฏูุ ุจุฒูุงุฏุฉ ูุฏุฑูุง ุฎูุณุฉ ูุนุดุฑูู ุจุงููุงุฆุฉ ููุงุฑูุฉ ุจุงููุชุฑุฉ ููุณูุง ูู ุงูุนุงู ุงููุงุถู. ุฌุงุก ูุฐุง ุงูุฅุนูุงู ุงูููู ุฎูุงู ุงููุคุชูุฑ ุงูุตุญูู ุงูุฐู ุนูุฏู ุงูุฑุฆูุณ ุงูุชูููุฐู ููุดุฑูุฉ ุงููููุฏุณ ุฃููู ุญุณู ุงููุงุตุฑ ูู ุงูููุฑ ุงูุฑุฆูุณู ููุดุฑูุฉ ุจูุฏููุฉ ุงูุธูุฑุงู. ูุฃูุถุญ ุงููุงุตุฑ ุฃู ูุฐู ุงููุชุงุฆุฌ ุชุนูุณ ููุฉ ุงูุฃุฏุงุก ุงูุชุดุบููู ููุฏุฑุฉ ุงูุดุฑูุฉ ุนูู ุงูุชููู ูุน ุชููุจุงุช ุงูุฃุณูุงู ุงูุนุงูููุฉ. ูุฃุถุงู ุฃู ุงูุดุฑูุฉ ููุนุช ุงุชูุงููุงุช ุดุฑุงูุฉ ุงุณุชุฑุงุชูุฌูุฉ ุถุฎูุฉ ูุน ุดุฑูุฉ ุชูุชุงู ุฅูุฑุฌูุฒ ุงููุฑูุณูุฉ ูุดุฑูุฉ ุดู ุงูุจุฑูุทุงููุฉ ูุชุทููุฑ ุญููู ุงูุบุงุฒ ุงูุทุจูุนู. ููู ุณูุงู ูุชุตู ุจุงูุงูุชุตุงุฏ ุงููุทููุ ุณุฌู ูุคุดุฑ ุงูุณูู ุงูุณุนูุฏู (ุชุฏุงูู) ุงูุฎูุงุถุงู ุญุงุฏุงู ุจูุณุจุฉ 2%ุ ูุชุฃุซุฑุงู ุจุชุฑุงุฌุน ูุทุงุน ุงูุจููู. ููู ุฎุทูุฉ ููุงุฌุฆุฉุ ุฃุนูู ูุตุฑู ุงูุฑุงุฌุญูุ ุฃุญุฏ ุฃูุจุฑ ุงูุจููู ุงูุฅุณูุงููุฉุ ุนู ุงูุฎูุงุถ ุทููู ูู ุฃุฑุจุงุญู ุงููุตููุฉ ุจุณุจุจ ุฒูุงุฏุฉ ุงููุฎุตุตุงุช.\"\"\",\n",
    "            'reference_summary': \"ุฃุฑุจุงุญ ููุงุณูุฉ ูุฃุฑุงููู ูุดุฑุงูุงุช ูุน ุชูุชุงู ูุดูุ ูุณุท ุชุฑุงุฌุน ุงูุณูู ุงูุณุนูุฏู ูุฃุฑุจุงุญ ุงูุฑุงุฌุญู.\",\n",
    "            'entities': [{'text': 'ุฃุฑุงููู ุงูุณุนูุฏูุฉ', 'label': 'ORG'}, {'text': 'ุฃููู ุงููุงุตุฑ', 'label': 'PERS'}, {'text': 'ุงูุธูุฑุงู', 'label': 'LOC'}, {'text': 'ุชูุชุงู ุฅูุฑุฌูุฒ', 'label': 'ORG'}, {'text': 'ุดู', 'label': 'ORG'}, {'text': 'ุงูุณูู ุงูุณุนูุฏู', 'label': 'ORG'}, {'text': 'ุจูู ุงูุฑุงุฌุญู', 'label': 'ORG'}],\n",
    "            'sentiment': 'mixed'\n",
    "        },\n",
    "        {\n",
    "            'text': \"\"\"ุงุฎุชุชูุช ุงูููุฉ ุงูุนุฑุจูุฉ ุงูุทุงุฑุฆุฉ ุฃุนูุงููุง ูู ุงูุนุงุตูุฉ ุงูุฃุฑุฏููุฉ ุนูุงูุ ูุณุท ุญุถูุฑ ุฑููุน ุงููุณุชูู ูู ูุงุฏุฉ ุงูุฏูู ุงูุนุฑุจูุฉ. ููุฏ ูููู ุนูู ุฌุฏูู ุงูุฃุนูุงู ุงููุถุน ุงููุชูุฌุฑ ูู ุงูููุทูุฉ. ูุฃูุฏ ุงูุนุงูู ุงูุฃุฑุฏูู ุงูููู ุนุจุฏุงููู ุงูุซุงูู ูู ูููุชู ุงูุงูุชุชุงุญูุฉ ุนูู ุถุฑูุฑุฉ ุงูุชุถุงูู ุงูุนุฑุจู ูููุงุฌูุฉ ุงูุชุญุฏูุงุช ุงูุฑุงููุฉ. ูุนูู ูุงูุด ุงูููุฉุ ุนูุฏ ุงูููู ุนุจุฏุงููู ุงุฌุชูุงุนุงุช ุซูุงุฆูุฉ ูุบููุฉ ูุน ููู ุงูุนูุฏ ุงูุณุนูุฏู ุงูุฃููุฑ ูุญูุฏ ุจู ุณููุงูุ ูุงูุฑุฆูุณ ุงููุตุฑู ุนุจุฏุงููุชุงุญ ุงูุณูุณูุ ุญูุซ ุชู ุจุญุซ ุณุจู ุชูุณูู ุงูููุงูู. ููุงูุดุช ุงูููุฉ ุจุงุณุชูุงุถุฉ ุงูุฃูุถุงุน ุงููุฃุณุงููุฉ ูู ุณูุฑูุง ูุงููููุ ุฏุงุนูุฉ ุงููุฌุชูุน ุงูุฏููู ุฅูู ุชุญูู ูุณุคูููุงุชู ูุฅููุงุก ุงูุตุฑุงุนุงุช ูููู ูุฒูู ุงูุฏู.\"\"\",\n",
    "            'reference_summary': \"ุฎุชุงู ุงูููุฉ ุงูุนุฑุจูุฉ ูู ุนูุงู ุจุฏุนูุงุช ููุชุถุงููุ ูููุงุกุงุช ุจูู ุงูููู ุนุจุฏุงููู ููุญูุฏ ุจู ุณููุงู ูุงูุณูุณู.\",\n",
    "            'entities': [{'text': 'ุนูุงู', 'label': 'LOC'}, {'text': 'ุนุจุฏุงููู ุงูุซุงูู', 'label': 'PERS'}, {'text': 'ูุญูุฏ ุจู ุณููุงู', 'label': 'PERS'}, {'text': 'ุนุจุฏุงููุชุงุญ ุงูุณูุณู', 'label': 'PERS'}, {'text': 'ุณูุฑูุง', 'label': 'LOC'}, {'text': 'ุงูููู', 'label': 'LOC'}],\n",
    "            'sentiment': 'neutral'\n",
    "        },\n",
    "        {\n",
    "            'text': \"\"\"ุชุดูุฏ ุงูููููุฉ ุงูุนุฑุจูุฉ ุงูุณุนูุฏูุฉ ุทูุฑุฉ ุชูููุฉ ูุงุฆูุฉุ ุญูุซ ุฃุนููุช ุฌุงูุนุฉ ุงูููู ุนุจุฏุงููู ููุนููู ูุงูุชูููุฉ (ูุงูุณุช) ุนู ุฅุทูุงู ูุจุงุฏุฑุฉ ูุทููุฉ ููุฐูุงุก ุงูุงุตุทูุงุนู ุจุงูุชุนุงูู ูุน ุดุฑูุงุช ุนุงูููุฉ ูุซู ุฌูุฌู ููุงููุฑูุณููุช. ูุชูุฏู ุงููุจุงุฏุฑุฉ ุฅูู ุชุนุฑูุจ ุชูููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุชุทููุฑ ููุงุฐุฌ ูุบููุฉ ุชุฎุฏู ุงูููุทูุฉ ุงูุนุฑุจูุฉ. ููู ุงููุทุงุน ุงูุตุญูุ ุญูู ูุณุชุดูู ุงูููู ููุตู ุงูุชุฎุตุตู ููุฑูุฒ ุงูุฃุจุญุงุซ ุฅูุฌุงุฒุงู ุทุจูุงู ุบูุฑ ูุณุจููุ ุญูุซ ูุฌุญ ูุฑูู ุทุจู ุจููุงุฏุฉ ุงูุฏูุชูุฑ ุณุนูุฏ ุงูุดูุฑู ูู ุชุทุจูู ุนูุงุฌ ุฌููู ูุชุทูุฑ ููุฑุถู ุงูุณุฑุทุงูุ ููุง ุฃุฏู ุฅูู ูุณุจ ุดูุงุก ุนุงููุฉ ุฌุฏุงู. ููุฏ ุฃุดุงุฏ ูุฒูุฑ ุงูุตุญุฉ ููุฏ ุงูุฌูุงุฌู ุจูุฐุง ุงูุชูุฏู ุงูุนููู ุงููุจูุฑ.\"\"\",\n",
    "            'reference_summary': \"ุฃุทููุช ูุงูุณุช ูุจุงุฏุฑุฉ ููุฐูุงุก ุงูุงุตุทูุงุนู ูุน ุฌูุฌู ููุงููุฑูุณููุช. ุญูู ูุณุชุดูู ุงูููู ููุตู ุงูุชุฎุตุตู ุฅูุฌุงุฒุงู ุทุจูุงู ุจููุงุฏุฉ ุณุนูุฏ ุงูุดูุฑู.\",\n",
    "            'entities': [{'text': 'ูุงูุณุช', 'label': 'ORG'}, {'text': 'ุฌูุฌู', 'label': 'ORG'}, {'text': 'ูุงููุฑูุณููุช', 'label': 'ORG'}, {'text': 'ูุณุชุดูู ุงูููู ููุตู ุงูุชุฎุตุตู', 'label': 'ORG'}, {'text': 'ุณุนูุฏ ุงูุดูุฑู', 'label': 'PERS'}, {'text': 'ููุฏ ุงูุฌูุงุฌู', 'label': 'PERS'}],\n",
    "            'sentiment': 'positive'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    UltimatePipeline().run(get_large_data())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
